# Inteligencia Artificial

Estado del arte de la Inteligencia Artificial  
P. 02

Contexto social de la IA  
P. 18

Potencial y escenarios futuros de la Inteligencia Artificial  
P. 36

Recomendaciones para IA  
P. 50

---



---

## Índice

######## 01

##### Estado del arte de la Inteligencia Artificial

1. Introducción: ¿Qué es la IA? ¿Por qué este ‘boom’ ahora?  
2. Aplicaciones. Así vivimos con la IA  
    1. Gestión y optimización de negocio  
    2. Marketing y publicidad  
    3. Salud y bienestar  
    4. Movilidad  
    5. Fintech  
    6. Otras áreas  

######## 02

##### Contexto Social de la IA

1. Impacto social de la Inteligencia Artificial. Riesgos y retos  
    1. Impacto en el comportamiento humano  
    2. Impacto en el empleo  
    3. Impacto en la justicia  
    4. Impacto en la gobernanza  
    5. Impacto en la privacidad  
    6. Retos técnicos  
2. Potenciales soluciones  
    1. Marco ético  
    2. Educación  
    3. Derechos digitales  
    4. Propiedad de datos  
    5. Inteligencia Artificial para el bien social

######## 03

##### Potencial y escenarios futuros de la Inteligencia Artificial

1. Potencial técnico  
    1. Ciencia de datos  
    2. Comprensión del lenguaje natural  
    3. Vehículos autónomos  
    4. Edge Computing  
    5. Computación cuántica  
2. Escenarios futuros    
    1. Empleo y sociedad  
    2. Educación  
    3. Salud  
    4. Administración pública  
    5. Gestión empresarial  
    6. Seguridad ciudadana  
    7. Escenarios extremos  

######## 04

##### Recomendaciones para IA

---

######## 01

## Estado del arte de la Inteligencia Artificial

#### Está en todas partes, casi omnipresente.

### Selecciona la música que escuchamos, las series que vemos, qué leemos, cómo nos movemos.

#### Puede ver, leer, escuchar y dar respuesta. Pero… ¿qué es realmente la Inteligencia Artificial?

######## 1.1

##### Introducción: ¿Qué es la IA? ¿Por qué este ‘boom’ ahora?

Responde a cada búsqueda online, a nuestras órdenes en casa y a las de Gobiernos, empresas y todo tipo de organizaciones. Lo mismo limpia el suelo que escribe noticias, traduce textos, recomienda estrategias de negocio o una buena dieta, analiza nuestra salud, o asiste en la conducción o en la selección de recursos humanos. Aprende de nosotros para mejorarse a sí misma. Es la Inteligencia Artificial, a la que a partir de ahora llamaremos IA. Pero ¿qué es la IA?

“¿Qué es IA? Me preguntas señalando con tu móvil mi pantalla azul. ¿Qué es IA? ¿Y tú me lo preguntas? IA eres tú”, podría responder el poeta romántico Gustavo Adolfo Bécquer. Y no estaría equivocado. La IA la hacemos cada uno de nosotros y nuestros datos, aunque -como veremos más adelante- es algo más que eso. Antes que Bécquer, otro poeta ya había discurrido sobre ello. Era el también español **[Ramón Llull](https://es.wikipedia.org/wiki/Ramon_Llull),** considerado un precursor de esta tecnología.

Este polímata medieval vivió en el siglo XIII y - además de poeta- fue filósofo, teólogo y hasta mártir declarado beato. Sus ideas religiosas le llevaron a la pionera concepción de una rueda mecánica pensante para validar o invalidar argumentos. Con ella se podría implementar el razonamiento de forma artificial en un artefacto con el que se podrían mostrar las verdades de la fe cristiana de una manera tan clara que no hubiese lugar a discusión. Una rueda capaz de demostrar que los dogmas de la fe cristiana eran correctos, como describió en su obra Ars Magna.  

También antes que Llull hubo otros. Hay quienes sitúan los orígenes de la IA en la Antigüedad. “Los primeros autómatas –robots antropomorfos– que imitaban movimientos humanos fueron construidos hace milenios. Según la Ilíada, Hefesto –el dios griego del fuego y la forja– creó dos mujeres artificiales de oro con ‘sentido en sus entrañas, fuerza y voz’ que lo liberaban de parte de su trabajo. Es decir, robots para ayudarlo, lo cual lo convierte en todo un adelantado a su tiempo”, relata nuestra experta e ingeniera **Nuria Oliver**

---

-participante en el Future Trends Forum que ha dado lugar a este informe- en su discurso de toma de posesión de su cargo en la Real Academia de Ingeniería de España (RAI).

Ya a mediados del siglo XX, uno de los considerados  
padres de la IA en su concepción moderna, **[Marvin Minsky](https://es.wikipedia.org/wiki/Marvin_Minsky),** la definía como “la ciencia de hacer que las máquinas hagan cosas que requerirían inteligencia si las hiciera un humano”. Desde entonces, esta tecnología ha sufrido varios inviernos [periodos de una marcada desaceleración en el interés y la inversión en IA] y algunas primaveras, hasta su florecimiento actual.

La **IA son básicamente algoritmos:** “son secuencias de pasos que en vez de ejecutarlos una persona los ejecuta una máquina”, explica en *National Geographic* José Manuel Molina, catedrático de Ciencia de la Computación e Inteligencia Artificial en la Universidad Carlos III de Madrid. “Dichos pasos -prosigue- no son lineales sino condicionales: se darán unos u otros según se cumplan o no ciertos requisitos. Si pasa X harán una cosa y, si pasa Y, harán otra”. La capacidad de aprender por sí sola de la IA se conoce como aprendizaje profundo, ya que los algoritmos funcionan como conexiones neuronales que se ajustan automáticamente. “La técnica consiste en crear un sistema en el ordenador que replique nuestro cerebro: un conjunto de neuronas interconectadas por pesos, que aprendan a funcionar como en un ser humano”, explica.

#### La explosión actual de la IA tiene que ver con el aprendizaje profundo y también con el aprendizaje automático

La explosión actual de la IA tiene que ver con el aprendizaje profundo y también con el aprendizaje automático, que -como lo define la multinacional de software **[SAS](https://www.sas.com/en_us/insights/analytics/machine-learning.html)-** automatiza la construcción de modelos analíticos mediante el análisis de grandes volúmenes de datos que permite

identificar patrones y tomar decisiones con la mínima intervención humana, aprendiendo por sí mismo.

---

Sin embargo, el hecho de que el sistema realmente aprenda es discutido. El experto en IA y tecnologías semánticas Sinuhé Arroyo, fundador de Taiger, niega y rechaza el término. Su argumento: que en el *machine learning* no hay aprendizaje, sino entrenamiento a base de datos. Esto es -explica- porque se enmarca en la parte no simbólica de la IA, cuya base es estadística, frente a la parte simbólica de representación de conocimiento y razonamiento automático, cuyo fundamento es la lógica. En esta última, en el campo de la semántica, sí hay aprendizaje. Es posible -dice- ampliar una base de conocimiento, añadir de forma automática nuevos conceptos, etc., y no se necesitan muchos documentos para realizar dicho aprendizaje.

Sea como fuere, los algoritmos de aprendizaje automático no son algo nuevo. Estos han existido durante mucho tiempo. Lo que es reciente es la capacidad de aplicar automáticamente cálculos matemáticos complejos a grandes datos, una y otra vez, cada vez más rápido. Las técnicas de IA disponibles actualmente son básicamente las mismas que hace treinta años. Lo que ha cambiado es la infraestructura y la disponibilidad de los datos.

Gracias a internet y al desarrollo de ordenadores más rápidos -que aceleran la capacidad de cómputo- podemos responder a nuevas necesidades y usar este tipo de técnicas que estaban disponibles antes, pero no eran tan relevantes como lo son hoy, tal y como explica la científica independiente experta en Inteligencia Artificial **[Alexandra Kirsch](https://www.alexkirsch.de/person-en.html)** en **[Xataka](https://www.xataka.com/robotica-e-ia/grandes-expertos-que-escepticos-inteligencia-artificial-puro-marketing-al-no-ha-avanzado-30-anos).** Precisamente por ello, es ahora cuando están emergiendo las aplicaciones más interesantes de la IA y aquellas con más proyección. Al mismo tiempo, el entusiasmo en torno a su potencial se ha disparado.

Los críticos cuestionan el nombre que se ha dado a esta tecnología, a la que no consideran inteligente o que incluso consideran que llamarle tal cosa es engañoso. Los expertos del foro recordaron que el desarrollo originario de la IA responde a la resolución de problemas, no a la búsqueda de inteligencia, por lo que cree que el campo debe denominarse “procesamiento complejo de información”.

**[Pablo Gervás](https://www.linkedin.com/in/pablo-gerv%C3%A1s-b013451/),** director del grupo de investigación en Interacción Natural basada en el Lenguaje y del Instituto Universitario de Tecnologías del Conocimiento de la Universidad Complutense de Madrid, tampoco cree que el nombre de IA sea muy acertado. Gervás señala que, si bien cada una de las capacidades de la IA por separado puede considerarse un ingrediente para la inteligencia, no lo son por sí solas. Si pensamos en los vehículos autónomos, no es que sepan conducir, sino que saben girar el volante como tienen que hacerlo para no salirse del carril, y algunas otras tareas básicas. “Hay que explicar a la gente que las máquinas no son como nosotros, que tenemos muchas habilidades y sabemos cómo combinarlas, cambiar de una a otra”, asegura.

#### La IA actual es limitada

En efecto, la IA actual es limitada. Es capaz de resolver muy bien una única tarea, una vez contextualizada. Sin embargo, es incapaz de sumar aprendizajes como las personas: un sistema de IA puede saber jugar muy bien al ajedrez, pero ser incapaz de encontrar un tumor en una imagen. Y, si aprende a identificar tumores, se olvidará de cómo jugar al ajedrez. Es lo que se conoce como ‘olvido catastrófico’.

**Ramón López de Mántaras** aclara que ningún programa o máquina ha pasado siquiera el test de Turing, que evalúa si una máquina puede tener un comportamiento indistinguible al de un humano por vía textual. Y también asegura que, aunque lo pasase, eso no significaría que es inteligente. “Remitirse únicamente a todo aquello que es verbalizable, que se puede expresar por el medio escrito, como son las preguntas del test de Turing, es delimitar muchísimo lo que es la inteligencia. La inteligencia es mucho más que la capacidad de mantener un diálogo coherente. En todo caso, es un test sobre aquellos aspectos de la inteligencia que puedan ser expresados mediante palabras”, explica.

López de Mántaras explica la IA como un sistema basado en la ‘competencia sin comprensión’, un postulado ampliamente compartido -y también discutido- en la comunidad científica, propuesto por el filósofo de la mente y científico cognitivo **[Daniel Dennett](http://ase.tufts.edu/cogstud/dennett/index.html).** Se refiere al hecho de que, a nivel funcional, un sistema puede alcanzar un nivel de rendimiento (competencia) que en contextos humanos se atribuiría a la comprensión (es decir, la inteligencia) pero sin comprenderlo. Cree que tal vez en unos años la IA sí sea capaz de entender, pero teme que estemos cerca de vivir un nuevo invierno en esta tecnología “por las sobre promesas que se están haciendo”.

El debate al respecto siempre encuentra posiciones enfrentadas. Hay, entre nuestros expertos, quienes van más allá de López de Mántaras y piensan que la IA nunca será capaz de convencer y que es necesario tenerlo claro para avanzar; otros opinan que tal vez no es que la IA no comprenda, sino que ha desarrollado su propio modelo de comprensión, diferente al humano; otros creen que, si hay

---

comprensión o no, no importa; para otros importa si el propósito de la IA es aprender e imitar un comportamiento inteligente.

Para el reconocido neurocientífico **Antonio Damasio,** la IA no es ni más ni menos que “una herramienta práctica para aumentar las capacidades y mejorar la naturaleza de los humanos, concebida por los humanos”. Cree que es parte de nuestra manera de evolucionar.

Virginia Dignum, miembro del Grupo de Expertos de Alto Nivel de la Comisión Europea sobre Inteligencia Artificial y profesora en el Departamento de Ciencias de la Computación en la Universidad de Umeå (Suecia), asegura que no hay tal cosa como una IA. “Hay sistemas computacionales que usan IA, que es un campo de estudio, una tecnología multipropósito, y no una entidad”, sostiene.

Las diferentes descripciones de los expertos dejan ver a la IA como constructo social, un concepto que parte de un deseo, que está vivo y que evoluciona en el tiempo, y que habla de cómo es el ser humano en sus dimensiones antropológica, social y cultural en cada época. Eso es precisamente lo interesante, esa amplitud de miras y visiones.

¿Qué es capaz de hacer esta herramienta? El catedrático de la Universidad de Valencia **[José Hernández-Orallo](https://www.linkedin.com/in/jose-hernandez-orallo-ba132b1/),** participante en el seminario HUMAINT del Centro Común de Investigación (JRC- CAS) de la Comisión Europea, enumera las áreas principales de la IA:

- Representación del conocimiento mediante ontologías, diferentes tipos de lógica o inferencia posible (X es un ave, luego X puede volar).
- Planificación y programación temporal y planificación probabilística.
- Aprendizaje automático: modelos lineales, árboles de decisión, redes neuronales.
- Reconocimiento de patrones.
- Visión artificial, reconocimiento facial, biometría.
- Procesamiento del lenguaje: reconocimiento del discurso, generación de lenguaje natural, resumen, recuperación, traducción, etiquetado, análisis de sentimientos, etc.

El inversor y gurú chino de la IA, Kai-Fu Lee, enmarca el desarrollo de este conjunto de tecnologías en **[cuatro diferentes olas](https://medium.com/@kaifulee/the-four-waves-of-a-i-46e7e627c054):**

1. Internet de la IA, una primera etapa de implementación alimentada por la gran cantidad de datos que fluyen a través de la web, que crean un perfil detallado de nuestras personalidades, hábitos, demandas y deseos: la receta perfecta para un contenido más personalizado para mantenernos en una plataforma determinada, o para maximizar los ingresos o ganancias.
2. Inteligencia empresarial, con una IA caracterizada por la capacidad de explorar las correlaciones ocultas que escapan a nuestra lógica lineal de causa y efecto y que puede superar incluso al más veterano de los expertos.
3. Percepción inteligente, que se actualiza con los ojos, oídos y miles de diferentes sentidos, que recopila nuevos datos que nunca se habían capturado, que son usados para crear nuevas aplicaciones. Sensores y dispositivos inteligentes como interfaces de voz o aplicaciones de visión artificial son algunos ejemplos.
4. IA autónoma, “la ola más monumental, pero también la más compleja”, según Lee. Esta integra todas las olas anteriores: da a las máquinas la capacidad de sentir y responder al mundo que las rodea, de moverse de forma intuitiva y de manipular objetos con la misma facilidad que un humano. Los vehículos autónomos son su máximo exponente por el momento.

De estas diferentes capacidades de la IA emanan multitud de aplicaciones posibles. ¿Qué es posible hacer ya? ¿Cómo la estamos usando e incorporando a los negocios y a la vida cotidiana, activa o pasivamente? De ello hablamos en el siguiente apartado.

######## 1.2

##### Aplicaciones. Así vivimos con la IA

Casi sin darnos cuenta, la IA se ha integrado en nuestro día a día. La llevamos en nuestro bolsillo, en nuestras pantallas. La tenemos en casa, en la escuela, en el trabajo e incluso en nuestros planes de ocio; el sector servicios, en la industria y en la agricultura. Con mayor  
o menor éxito, muestra sus armas en el camino **hacia la personalización en sectores** como la medicina, la educación e incluso la belleza y, en general, el consumo. **También persigue la optimización,** ya sea del tráfico, de la cadena de suministro, de los cultivos; del **rendimiento empresarial** mental y físico. Es incluso capaz de componer música o generar creaciones artísticas.

---

Para **Wilfried Vanhonacker,** cofundador y exdecano de **[CEIBS](http://www.ceibs.edu/)** (Escuela Internacional de Negocios China-Europa) y fundador de **[SKOLKOVO](http://sk.ru/news/),** Escuela de Administración y Dirección de Empresas ubicada en Moscú (Rusia), la IA y el big data en conjunto no solo están dando lugar al mayor mercado emergente hasta la fecha, sino que van mucho más allá. “Cambiarán fundamentalmente la mayoría de las industrias. Todos los procesos de negocio serán inteligentes”, afirma. Cree que el descubrimiento de nuevos fármacos y materiales será el ámbito con mayor potencial económico. Más allá de los negocios, apunta a su uso para seguridad, defensa y vigilancia.

Gestión y optimización de negocio, marketing y publicidad, salud y bienestar, movilidad, finanzas junto con los ámbitos legal y policial, la logística y el trabajo son las áreas de aplicación de la IA destacadas por los expertos en el foro:

######## 1.2.1

###### Gestión y optimización de negocio

“Gastaré el dinero del Nobel de la forma más irracional posible”, dijo **[Richard Thaler](https://es.wikipedia.org/wiki/Richard_Thaler)** a The New York Times tras conocerse su nombramiento como Premio Nobel de Economía de 2017. Este profesor distinguido de Economía y Ciencias del Comportamiento de la Escuela de Negocios Booth de la Universidad de Chicago (Estados Unidos) fue precisamente reconocido por sus investigaciones sobre cómo los aspectos emocionales y no racionales influyen en la toma de decisiones económicas y financieras. Es la economía conductual o del comportamiento, que trata de entender cómo el ser humano toma decisiones como agente económico, partiendo de la base de que no siempre lo hace de forma óptima.

Antes que Thaler, el psicólogo **[Daniel Kahneman](https://es.wikipedia.org/wiki/Daniel_Kahneman),** ganó también el Nobel por su estudio de la economía conductual en el marco de un nuevo campo: la neuroeconomía.  
Como sugiere la investigación de Kahneman y Thaler, las personas, tanto individuos como parte de grupos, siguen todo tipo de atajos cognitivos cuando toman decisiones, compras e inversiones”. En otras palabras, “lo que la gente realmente hace tiende a ser mucho más revelador que lo que la gente dice”, explica Michael Schrage, experto del Future Trends Forum, investigador del **Centro para Negocios Digitales de la Escuela de Negocios SLOAN del MIT** (el

Instituto Tecnológico de Massachusetts) y autor de varios libros.

El trabajo de Schrage se centra en la economía conductual de los modelos, prototipos y experimentos como medio para gestionar los riesgos y oportunidades de la innovación. En el caso de la IA, cómo se puede usar para ofrecer a la gente no solo mejores y más oportunas recomendaciones y consejos, sino también más persuasivos. Estas recomendaciones se pueden emplear para optimizar el trabajo y su gestión, para reducir la incertidumbre y para aumentar las ventas.

“El impacto que tienen la IA y el aprendizaje automático en un entorno de datos enriquecidos es que se convierten en el centro de gravedad y el punto clave en la transformación de las capacidades de negocio”, asegura el economista. Se refiere a la creación de valor, a la reducción de costes, a la experiencia del usuario, etc. “Las capacidades constituyen la perspectiva con la que deberíamos observar cómo se verá afectada la gestión de aquí en adelante”, señala.

Su tesis: que el factor esencial es la capacidad de aprender a optimizar. La optimización es el origen de la diferenciación en cuanto a qué significan la gestión y el liderazgo en un entorno de IA. Este es uno de los hallazgos clave que Schrage ha detectado junto con sus compañeros del MIT, tras un extenso análisis. Esto obliga a revisitar las nociones básicas; no tiene que ver con cómo mejoramos, simplificamos o sintetizamos nuestras capacidades ni de cómo las hacemos más eficientes, sino con cuál queremos que sea el resultado.

---

“Tenemos como ejemplos a seguir en Netflix, Tencent, Google y Amazon, que están centradas en los resultados, ya sea en el campo de la reducción de costes, de la creación de valor… ¿Cuál queremos que sea el resultado?”, repite. Para el investigador, esa pregunta va a ser motivo de tensión entre la gobernanza, el liderazgo y la gestión, porque el debate va a girar en torno a ello. En un mundo con ética, se convierte en el verdadero problema de la IA para las empresas de todo el mundo (particularmente en China). “Todos tendremos que justificar cuál es el propósito, cuál es el porqué para decidirnos a optimizar unas dimensiones en lugar de otras”, afirma.

¿Cuál es, a su juicio, la verdadera conclusión práctica y realista de esto? (con la que algunos expertos del think tank discrepan): los KPI [medidores de desempeño] son la estrategia, y la estrategia son los KPI. Sin KPI no financieros que sean explícitos y que definan qué es lo que se va a optimizar, no hay estrategia. Según sus estudios, además, el aprendizaje automático ha motivado un cambio enorme en esos KPI: han pasado de ser resultados que ayudaban  
a la gente a tomar decisiones a utilizarse como insumos para las máquinas. Por ello, cree que los KPI no se deberían considerar métricas, sino actores de software inteligentes que quieren aprender a optimizar.

#### Los KPI son la estrategia y la estrategia son los KPI

De vuelta al debate de la competencia versus la comprensión: quizás esos sistemas no sepan por qué hacen lo que están haciendo, pero están programados para aprender a optimizar. Y de ahí la importancia de elegir los KPI adecuados. Si las máquinas van a tomar decisiones, ¿cómo cambia eso los indicadores y qué tipo de datos se necesitan? A juicio de Schrage, a mejor rendimiento,

mejores datos y mejores decisiones. Y esa debe ser -en su opinión- la manera en la que las empresas deben gestionar las aplicaciones de IA.

#### A mejor rendimiento, mejores datos y mejores decisiones

A esto hay que añadir otro componente: cómo crear ciclos virtuosos entre los KPI y el entorno digital en que se encuentran. “El pecado original de las empresas de legado es que no entienden en absoluto cómo hacerlo. Pueden dividir en compartimentos, visualizar, crear silos… pero se les da muy mal crear ciclos virtuosos. Por eso les va tan mal en los mercados bursátiles y en la competición contra empresas digitales, porque Facebook, Uber y todas estas organizaciones entienden el efecto de red”, asegura.

Para el investigador, es un hecho clave: hay que invertir en los ciclos virtuosos. Cree que ello va a obligar a revisar la antología de la optimización y  
qué significa y significará esta no solo ahora, sino dentro de tres o cinco años. Es el nuevo idioma de la planificación estratégica -dice- en un contexto de empresas que se están pasando a los microservicios y que antepone los KPI. “El futuro de la optimización es el futuro del aprendizaje automático y la IA, y el futuro de la IA y el aprendizaje automático será el futuro de la optimización. Ese es el futuro de la gestión”, concluye.

#### El futuro de la gestión depende de la optimización del aprendizaje automático y la IA

Para **David Weinberger,** tecnólogo e investigador del centro **[Harvard Berkman Klein para Internet y la Sociedad](https://cyber.harvard.edu/),** la clave en el contexto actual de cambio, volatilidad y ambigüedad es cómo manejar la incertidumbre. Este filósofo y escritor -residente en el programa **[Google PAIR](https://ai.google/research/teams/brain/pair)** de Inteligencia Artificial- señala que hemos adoptado técnicas y comportamientos que solo tienen sentido en un entorno conectado, digital y abierto.

---

En el ámbito de los negocios, Weinberger señala que hemos descubierto que a menudo podemos tener éxito al abstenernos de anticipar lo que sucederá. Ejemplo de ello son las plataformas abiertas creadas por las compañías para que cualquier desarrollador pueda aprovechar sus productos y servicios, extenderlos a nuevos usos e integrarlos en otros productos que sus clientes utilizan en sus flujos de trabajo. “Las organizaciones crean estas plataformas abiertas porque saben que no pueden anticipar lo que un mundo de usuarios conectados querrá hacer con sus productos”, sostiene.

Otro ejemplo desde el ámbito de las empresas: lanzar productos con un conjunto mínimo de características, de modo que no haya necesidad de tratar de predecir lo que sus usuarios querrán hacer con sus productos. Ambos son ejemplos de la falta de anticipación que, para el filósofo, es un sello distintivo de nuestras tácticas y comportamientos *online*. Sin embargo, no ha sido así en el mundo *offline* hasta ahora. “Abstenerse de anticipar contradice la que ha sido nuestra estrategia más básica desde los tiempos del Paleolítico. El éxito de la no anticipación nos ayuda a reconocer la imprevisibilidad y la naturaleza caótica de nuestro mundo”, afirma.

En este contexto, los modelos de aprendizaje automático ayudan a conceptualizar dicho caos, ya que no se basan en la reducción de la situación compleja a un puñado de factores conocidos y principios generales, sino que funcionan mediante la conexión de grandes cantidades de datos en correlaciones estadísticamente significativas. El resultado -sostiene Weinberg- puede generarse a partir de una red de influencias tan complejas y delicadas a las que el pensamiento humano simplemente no puede acceder. “El éxito de estos modelos, muchos de ellos para nosotros inexplicables, nos permite reconocer y abrazar la naturaleza caótica y contingente de nuestro mundo”, asegura.

######## 1.2.2

###### Marketing y publicidad

Si bien veíamos cómo Weinberger hablaba en el apartado anterior sobre el éxito de la no anticipación y la necesidad de abrazar el caos, la inclinación paleolítica a predecir sigue estando presente en los negocios y continúa modelando parte del desarrollo tecnológico.

“El *marketing* está obsesionado con el aprendizaje automático, que tiene que ver con el reconocimiento de patrones y las predicciones basadas en datos”, afirma **Adam West,** director de Marketing de **[Satalia](https://www.satalia.com/),** compañía de IA que combina la optimización y el aprendizaje automático para resolver problemas de difícil optimización.

---

West destaca que hay ciertas predicciones que a estos sistemas se les dan muy bien. Por ejemplo, con datos sobre el volumen de helados vendidos y la temperatura, el aprendizaje automático puede predecir cuántos se venderán al día siguiente. Sin embargo, no es tan bueno- el directivo- a la hora de resolver problemas de asignación de recursos o de tomar decisiones. “No te va a decir, basándose en su predicción, los helados que tienes que fabricar, la gente que tienes que contratar o lo que tienes que gastarte en publicidad”, afirma.

#### Hay ciertas predicciones que a los sistemas de aprendizaje automático se les dan muy bien

¿Por qué sucede esto, de acuerdo con West? Porque esa es la función de la optimización, un conjunto de destrezas técnicas “completamente diferente, no tan sexi como el aprendizaje automático y que no emociona tanto a la industria”. La buena noticia, acorde a lo que han observado en Satalia, es que el verdadero valor radica en la combinación de ambas.

Es cierto -dice West- que el aprendizaje automático tiene muchísimas aplicaciones dentro del área del marketing. Es posible comprobar el comportamiento previo del consumidor y hacer una predicción de cuándo va a dejar

de ser cliente de una determinada empresa, predecir el próximo producto que van a comprar y ofrecer un sistema de recomendación de productos, predecir la demanda de una tienda en concreto… “Todo eso está genial, las predicciones son estupendas, pero lo realmente valioso es cómo se aprovechan y qué decisiones se toman con ellas”, asegura.

#### El aprendizaje automático tiene muchísimas aplicaciones dentro del área del marketing

Algunos ejemplos de ello: si la predicción dice que alguien dejará de ser cliente, se debería usar dicha información para actuar y tratar de evitar que suceda. Lo mismo pasa con la predicción sobre la demanda que tendrá una tienda, que permite optimizar el inventario de productos para maximizar la experiencia del cliente y adecuar la cantidad de personas que trabajan atendiendo a la gente que causa esa demanda. Sobre esta base, en Satalia han ayudado al enrutamiento de vehículos para la cadena de supermercados Tesco o la optimización de la fuerza laboral de PwC.

“Estamos obsesionados con las predicciones, pero no se nos da demasiado bien combinar dichas predicciones con la analítica prescriptiva con el propósito de optimizar”, insiste West. Cree que es precisamente en este ámbito donde habrá más cambios en cuanto a cómo crean valor las empresas. El experto cita también al Nobel Daniel Kahneman: creemos que, como humanos, se nos da bien tomar decisiones y resolver estos problemas de asignación de recursos, pero la realidad es que intervienen en ellas muchísimos sesgos.

Precisamente por eso, a la hora de aprovechar las predicciones del aprendizaje automático tomamos malas decisiones. A esto contribuye también -dice el directivo de Satalia- el hecho de que estos problemas son exponencialmente complejos. ¿Cuál es la solución? Centrarnos no solo en el aprendizaje automático, sino en su combinación con la optimización y con los algoritmos de optimización.

Con respecto a la publicidad, West sostiene que ya no se invierte tanto en esta, sino en la capacidad operativa. El motivo: la forma construir una marca está cambiando. Tradicionalmente, un producto mediocre podía invertir mucho en publicidad, en el empaquetado y el *merchandising* para

---

envolverlo en asociaciones emocionales y sociales, en las que se basaba su diferenciación.

“Así es como se han construido las marcas, por norma general, en los últimos 30 o 40 años, así que ni Coca-Cola ni Pepsi compiten por el beneficio funcional del agua con azúcar, sino por las asociaciones con la marca. Es el mismo caso que Nike y Adidas, no compiten por la ropa (que es igual) sino por las asociaciones con la marca”, afirma el experto. Cree, sin embargo, que esto está cambiando porque ya no es tan fácil emular los beneficios funcionales de un producto.

West cita a “los Amazon y los Google del mundo” y cómo estos, aunque siguen teniendo asociaciones, no las han logrado invirtiendo directamente en ellas, sino en la innovación del producto, en la investigación y el desarrollo. “Ahora el beneficio funcional es superior, y así es como se diferencian”, afirma. Se produce así una transición a la hora de crear valor. Las marcas pasan de usar la publicidad y la comunicación a centrarse directamente en innovación del producto. Esto, al contrario que con los casos anteriores, no es fácil de emular, dada la dificultad de acceder al talento necesario para construir ese tipo de capacidades.

#### Las marcas pasan de usar la publicidad y la comunicación a centrarse directamente en innovación de producto

######## 1.2.3

###### Salud y bienestar

Hemos hablado en anteriores apartados de la economía conductual y de cómo esta puede usarse para influir en la toma de decisiones de las personas, basándose en los sesgos cognitivos conocidos. ¿Y si se usasen dichos sesgos para ayudar a la gente a cambiar malas costumbres y a adquirir hábitos y conductas para una vida más saludable y satisfactoria?

“Cambiar hábitos es difícil. La ciencia del comportamiento y otras ciencias cognitivas ayudan porque nos permiten entender qué es lo que dificulta esos cambios”, afirma **Oliver Smith,** director de Estrategia en el área de Salud de **[Telefónica Alpha](https://www.alpha.company/).**

Smith prefiere no hablar de ‘malos hábitos’ *per se*. Cree que estos surgen porque los hemos encontrado de alguna manera útiles o “algo bueno en algún punto, ya sea inconsciente o conscientemente”. El problema es cuando estos pueden conducir a una mala salud, lo cual a menudo también hace que una persona sea menos feliz. El experto explica que las ciencias del comportamiento pueden ayudar a responder preguntas como por qué nos resulta difícil depender de la fuerza de voluntad cuando estamos a dieta, o por qué la presión de grupo (explícita o percibida) a menudo nos impide abstenernos de tomar alcohol.

Para poder aplicar estas herramientas -y este es el desafío- es necesario poder medir lo que está sucediendo en el momento preciso y darle sentido. Aquí entra en

---

juego la IA, que se alimenta de los datos sobre nuestros comportamientos que registran nuestros teléfonos móviles y que esta tecnología permite analizar con mayor sofisticación, detectando patrones que de otra manera podrían eludirse. “En Alpha Health creemos que esta combinación nos permitirá desarrollar recomendaciones y servicios que se adapten a cada individuo, desde la prevención hasta el tratamiento, para ayudar a las personas a cambiar de hábitos”, afirma.

Su enfoque inicial es la salud mental y el bienestar.  
Para este grupo, la promesa de una mejor salud en el futuro lejano rara vez es lo suficientemente convincente para que los cambios de conducta se mantengan en el tiempo. Por ello- dice- es fundamental comprender qué es lo más motivador a nivel individual. El directivo aclara que no pretenden sustituir la labor de un médico, sino complementarla, centrándose en el aspecto del cambio de comportamiento.

“El 75% de las enfermedades crónicas se derivan de nuestro estilo de vida diario: lo que comemos, cómo dormimos, lo que bebemos”, afirma. En Alpha Health creen que al combinar esto con un conocimiento creciente sobre cómo se comporta alguien, por qué toma ciertas decisiones, qué porcentaje sucede de manera inconsciente, etc., entendiendo la neurociencia, la psicología y la psiquiatría detrás de ello, es posible crear un asistente sanitario

personal que realmente ayude a la gente a cambiar su conducta. El factor que falta en la ecuación es lo que Smith llama “el fenotipo digital”, los datos que permiten entender la personalidad de alguien y aconsejar a cada cuál cómo proceder en su caso concreto, en cada circunstancia, basándose en quién es y con quién está.

#### El fenotipo digital son los datos que nos permiten entender la personalidad de alguien

Smith cree que el ámbito de los asistentes sanitarios personales “explica bastante bien hacia dónde va la IA en materia de salud”. También considera que el ámbito de las predicciones “tiene muchísimo potencial”. En un análisis de historias clínicas anonimizadas de un hospital psiquiátrico, realizado conjuntamente por Alpha Health y el Servicio Nacional de Salud (NHS) de Reino Unido, fueron capaces de predecir con dos semanas de antelación y un 80% de precisión crisis de ansiedad o de otro tipo que podrían padecer los pacientes, unos resultados que Smith asegura que publicarán próximamente.

Otra área en la que se está trabajando mucho es la de la telemedicina que, si bien no usa IA, puede ser “un caballo de Troya” [en el buen sentido] para la IA. “Lo que buscan realmente las empresas que quieren que el paciente se comunique con su médico por teléfono, o que reciba un diagnóstico a través de una videollamada o similar, son sus datos para hacer otras cosas que sí utilizan la IA. Y ahí es donde podemos hacer algo que realmente cambie las reglas del juego”, asegura.

Queda para el final el que tal vez sea el ámbito más amplio de la aplicación de la IA en salud: los diagnósticos y predicciones. Son aprendizaje automático puro y duro: extraer un vasto volumen de datos, desarrollar un algoritmo y decirle “esto es lo que deberías hacer”. Suena fácil, pero no lo es tanto. Que se lo digan a Watson, el sistema ‘cognitivo’ de IBM que se ha dado de bruces con los enormes retos técnicos y humanos de llevar la IA a la práctica clínica, como explica **[un profuso artículo](https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-overpromised-and-underdelivered-on-ai-health-care)** en la revista *Spectrum* del Instituto de Ingeniería Eléctrica y Electrónica de EE.UU (IEEE).

El investigador **[Eric Topol](https://es.wikipedia.org/wiki/Eric_Topol),** profesor, director y fundador del Scripps Research Translational Institute, afirma en un **[artículo](https://www.nature.com/articles/s41591-018-0300-7)** en *Nature Medicine* que casi todos los médicos dedicados a la práctica usarán la IA en el futuro. Sin

---

embargo, advierte que, si bien el campo es ciertamente prometedor, hay relativamente pocas pruebas de esas promesas. También que el riesgo de dar con algoritmos defectuosos es exponencialmente más alto que el de una sola interacción médico-paciente, pero la recompensa en reducción de errores, ineficiencias y costes es considerable. “Por tanto, no puede haber excepcionalismo para la IA en medicina: requiere estudios rigurosos, publicación de los resultados en revistas revisadas por expertos y validación clínica en un entorno del mundo real, antes de su despliegue e implementación en la atención al paciente”, concluye.

######## 1.2.4

###### Movilidad

Primero fue el GPS, esa aplicación imprescindible en nuestro smartphone que parece tan básica y tan sencilla de usar y que es producto de la IA. No se limita a darnos instrucciones por vía textual o sonora, sino a advertirnos del tráfico o de problemas e imprevistos en el trayecto, a los que se adapta para configurar nuevas rutas. Eso que a escala individual es tan útil puede serlo tanto más para la gestión general del tráfico en las ciudades, con sistemas de transporte que se ajustan a la demanda en tiempo real.

Ahora le sumamos los vehículos autónomos, que buscan que este proceso se realice con la máxima eficiencia. Una prometedora aplicación de la IA a la movilidad que aún está emergiendo y que apenas da sus primeros pasos en la traslación de estos vehículos a la ciudad. “Cuando empecé a trabajar en esto hace 13 años, en Stanford en el 2006, había dos grupos: el grupo de Sebastian Thrun y el mío, y cada uno construía un coche y nos centrábamos en cosas diferentes. Sebastian buscaba la seguridad, yo hacer realidad el concepto de carsharing [coche compartido]”, recuerda **Raúl Rojas,** que lleva 20 años trabajando en robótica móvil, 13 en coches autónomos y 30 en Inteligencia Artificial, ahora desde su posición como investigador y profesor en la Universidad Libre de Berlín (Alemania).

Para Rojas, el futuro -ya presente- del transporte serán los teléfonos móviles. “No necesitas tener un coche de tu propiedad, ni lavarlo ni aparcarlo en el garaje, sino que llamas a un taxi. Es la mejor definición que tengo para los coches autónomos: es como llamar a un taxi cuyo conductor

es un ordenador”, afirma. Rojas cita varias simulaciones sobre cuántos coches se pueden ahorrar en carretera si los compartimos. Por ejemplo, la que hicieron en Berlín, que dio como resultado un ahorro estadístico del 90% de los coches de la red de transporte de la ciudad mediante carsharing, con vehículos que cooperan con el transporte público.

Rojas comenta cómo, si antes el foco estaba puesto en el conductor, en un futuro no muy lejano lo estará en el ordenador, que ahora ejerce de copiloto. “No muchos conocen esta idea de que el ordenador es el copiloto, siempre creen que son muy buenos conductores y nunca tienen accidentes, pero eso es porque el ordenador les ayuda”, afirma. Se refiere, entre otros, a los programas de estabilidad en las curvas de las autopistas. Cree que lo que va a seguir desarrollándose es este concepto de copiloto informático, antes de ver el despliegue del coche autónomo.

#### Antes que el coche autónomo, se avanzará con el concepto de copiloto informático

Los retos a los que los diseñadores y fabricantes de coches autónomos deben hacer frente son complejos. “Conducir en Arizona es fácil, pero intentar conducir en México no tiene nada que ver. Adelantar en las autopistas mexicanas es difícil porque no hay margen de error, no hay espacio ni a la derecha ni a la izquierda. Y hay muchas cosas más. Por ejemplo, calles sin carriles en las que el coche tiene que estimar lo mejor posible cuál es la manera más adecuada de proceder”, explica. Y esto es sólo una pequeña parte del problema.

¿Cómo resolverlo? “Del mismo modo en que se hace una gran parte de la Inteligencia Artificial. Por fuerza bruta”,

---

afirma. Como ejemplo, su prototipo, el coche ‘autoNOMO’, que cuenta con una gran cantidad de sensores, una redundancia multiplicada por cuatro, nueve cámaras de vídeo, ocho radares, seis escáneres de láser en la parte de abajo del coche y uno arriba. “Todo eso nos proporciona muchísima información, que se verá suplementada en el futuro por las telecomunicaciones”, afirma.

Sin embargo, este enfoque de fuerza bruta -reconoce Rojas- se viene abajo cuando hay que usarlo en un contexto de convivencia con coches conducidos por personas. El investigador recuerda una experiencia de conducción autónoma en Berlín tras una nevada. “Todo estaba blanco. AutoNOMO sabía exactamente dónde estaban los carriles porque tiene posicionamiento 3D basado en las características del entorno y un GPS muy preciso, pero la gente no sabía dónde estaban los carriles. La primera persona que condujo un coche cogió un supuesto carril y el resto la siguió, y al final autoNOMO estaba a unos dos metros del resto de conductores. AutoNOMO estaba haciendo lo correcto, situado en medio del carril, pero como todos los demás estaban separados, nosotros éramos los atípicos, los que íbamos a tener un accidente”, comenta.

Cuando se toparon con este problema, comenzaron a desarrollar lo que denominan ‘la conducta enjambre’. AutoNOMO ya no sólo comprueba el mapa y los sensores, sino también el comportamiento del resto de conductores que le rodean para poder adaptarse a él como lo hacen biológicamente entre sí los pájaros o los bancos de peces. “Hay reglas para esas conductas enjambre y eso significa que, en este caso, hay que prescindir de las reglas válidas para conducir en la autopista y desarrollar algo nuevo que tenga en cuenta el comportamiento externo. Creo que esto demuestra que en algún momento hay que complementar este enfoque de fuerza bruta con algo más inteligente”, señala Rojas.

#### AutoNOMO, además del mapa y los sensores, comprueba el comportamiento del resto de conductores

Estos problemas son aplicables al espacio terrestre, pero en el mar y en el aire surgen retos diferentes. Cada vez encontramos más tipos de drones con funciones diversas, desde salvamento hasta transporte de mercancías y todo tipo de productos. Estamos asistiendo a los comienzos del

desarrollo de vehículos voladores de transporte privado sin conductor, con empresas como Uber o Airbus que ya disponen de prototipos que incluso están probando en espacios controlados. En Ámsterdam se proponen llenar los canales de botes autónomos. Todas estas aplicaciones, que no serían posibles sin IA y *big data*, están ya aquí. Su implementación y despliegue a gran escala, no obstante, requerirá aún de varios años.

---

######## 1.2.5

###### Fintech

2018 fue un año estelar para el sector de las tecnologías financieras -las *fintech*- con más de 1.700 acuerdos por un valor de casi 40.000 millones de dólares y 39 ‘unicornios’ respaldados por firmas de capital riesgo, según un **[informe](https://www.cbinsights.com/research/report/fintech-trends-2019/)** de CB Insights. El sector seguirá creciendo. Según un **[análisis](https://home.kpmg/xx/en/home/insights/2019/02/fintech-predictions-2019.html)** de KPMG, 2019 es el año de la consolidación en áreas maduras como los pagos y préstamos, y también en áreas emergentes como blockchain. También predicen una mayor claridad regulatoria. Y, según el **[World Fintech Report 2019](https://www.capgemini.com/news/world-fintech-report-2019/)** de Capgemini, los datos serán cada vez más una ventaja y un activo crítico.

Para KPMG, 2019 es también el año de la maduración de la IA en el sector. ¿Cómo puede la Inteligencia Artificial añadir valor al sector? “El crecimiento en fintech lo motivarán,  
por una parte, la regulación de la Inteligencia Artificial y los datos y, por otra, la competición estratégica”, afirma **Gary Ang,** director de Estrategia en **[Temasek](https://www.temasek.com.sg/en/index.html),** compañía de inversión global -propiedad del gobierno de Singapur- con un portafolio de cerca de 200.000 millones de euros.

Ang sostiene que cualquier empresa *fintech* que sea sostenible y tenga éxito debe tener un modelo de negocio basado en la adquisición constante de datos. Estos se obtienen generalmente de dos maneras: a través de la adquisición de un cliente o, una vez adquirido, vendiéndole productos y servicios que generen más datos. “Una empresa *fintech*, desde la perspectiva de la IA, es tan buena como digan los datos que tenga”, afirma.

#### Una empresa fintech, desde la perspectiva de la IA, es tan buena como digan los datos que tenga

El experto señala que los datos que adquiere en el proceso -y no las comisiones asociadas con las transacciones- son su activo más valioso. Y cita a Ant Financial, el brazo *fintech* de Alibaba, que constituye la empresa más grande del sector, que dice que el 90% de su valor añadido radica en la adquisición de datos (con lo cual apenas el 10% procede de transacciones).

¿Cómo crear valor añadido con ello? Utilizando la IA para crear servicios completamente nuevos que la banca tradicional no ofrece. “El 50% de los ciudadanos chinos tienen una cuenta AliPay o de WeChat, sistemas de pago móviles que alcanzan un valor de 17 trillones de dólares estadounidenses anuales”, comenta Ang, que destaca además que estos gigantes *fintech* se han convertido en importantes instituciones financieras para el gobierno chino en apenas 10 años.

Otra empresa *fintech* innovadora que resalta el directivo de Temasek es Square (EE.UU.). Esta comenzó su andadura ayudando a pequeños comercios que no podían aceptar pagos con tarjeta de crédito, proporcionándoles un sistema de pago móvil. Rápidamente evolucionó, basando su negocio en la provisión de servicios de software y préstamos al sector no bancarizado. Además, la empresa está en proceso de obtener una licencia bancaria. Con su analítica de datos, han creado productos que antes no existían, haciendo que los clientes dependan más de Square y de su ecosistema.

Precisamente la creación de un ecosistema que actúe como un banco completamente digital es el segundo principal valor añadido en *fintech* al que hace referencia Ang. Ya sea en China o en Estados Unidos, las empresas *fintech* más exitosas como Ant y Square quieren escalar para aumentar su alcance. Al mover grandes cantidades de dinero y desafiar los dominios de la banca tradicional se convierten también en bancos digitales, momento en el que están sujetas a la regulación. Estas -dice el experto- pueden regularse mediante las métricas de datos tradicionales, pero también mediante nuevas métricas basadas en IA.

Ang comenta que en Singapur están experimentando, bajo un sistema *sandbox* [creación de espacios seguros

---

para la experimentación de innovaciones y tecnologías], cómo llevar esto a cabo. Sostiene que la regulación tendrá un fuerte impacto en los ganadores y los perdedores, ya que está motivada a nivel nacional y estratégico de Estados nación que compiten los unos con los otros. “Las empresas *fintech* en auge tendrán que tener cuidado con qué mercados se mantienen abiertos y también a la hora de sortear las regulaciones para abordar las inquietudes nacionales y estratégicas subyacentes”, afirma.

En opinión del directivo, hay dos componentes clave en la regulación de empresas fintech. El primer aspecto es la regulación tradicional, donde ya se han establecido estándares globales. El segundo aspecto importante es la moneda de la confianza, clave para que el consumidor comparta sus datos. Los servicios financieros tradicionales -incluidos los bancos minoristas incumbentes y la banca de seguros- que se enfrentan a la amenaza de la disrupción, se han sumado al recurso estratégico de la confianza. “Su debate se empieza a centrar no ya en qué es posible hacer con los datos, sino en qué se debería hacer con ellos, y tienen todo a su favor”, asegura Ang.

Sin confianza, es poco probable que los consumidores se entusiasmen con el concepto de banca abierta en un modelo en el que los datos de los clientes de los bancos tradicionales se compartan con las empresas *fintech*. “Aunque estas empresas han hecho un buen trabajo a la hora de proporcionar servicios de bajo coste a los no bancarizados, aún no se sabe lo sostenible que es este modelo, ya que salen a la luz muchos ejemplos de fugas de datos y ciberataques”, señala el experto.

Aquí es donde el uso ético de los datos y la IA desempeñan un papel fundamental, como veremos en el próximo capítulo.

######## 1.2.6

###### Otras áreas

Durante el foro, los expertos mencionaron también otras áreas de aplicación de la Inteligencia Artificial. En el ámbito legal, el profesor e investigador especializado en Leyes e Inteligencia Artificial en la **[Universidad de Cambridge](https://www.cam.ac.uk/)** (Reino Unido), **Christopher Markou,** ofrece su análisis de cómo está cambiando la IA el sistema jurídico, que tradicionalmente se ha resistido ante los cambios tecnológicos. “Nunca hubo una motivación real en el ámbito del derecho para adoptar nuevas tecnologías. Este siempre se ha basado, por lo general, en un modelo de facturación por horas. Por tanto, trabajar más rápido significaba ganar menos dinero, por lo que los grandes bufetes no tenían incentivos para adoptar las nuevas tecnologías y acelerar los procesos”, explica.

#### Existen múltiples áreas de aplicación de la IA en las áreas jurídica y policial

La IA está cambiando esa dinámica. En primer lugar -explica Markou- la estructura de incentivos ha cambiado: ahora a las empresas les cuesta resistirse al uso de la IA para ayudar con los asuntos jurídicos, incluso con la administración judicial. En algunos sitios, como en Estonia, incluso se están probando robots que reemplacen a los abogados y jueces humanos. “La Inteligencia Artificial va a cambiar el concepto y la práctica del derecho, ya lo ha cambiado. Pero lo que creo que va a ocurrir en el futuro, y lo que a la gente como yo nos preocupa, es el reemplazo de los jueces y abogados humanos mediante sistemas informáticos que quizás no compartan ninguno de los valores de nuestras leyes y sistemas jurídicos”, subraya.

**Lauren Dyson,** científica de datos en Civis Analytics, destaca las aplicaciones de la IA en ámbito policial: “En la mayoría de las comisarías de EE.UU. se utilizan algoritmos predictivos para decidir a dónde destinan a los oficiales de policía con el fin de prevenir el mayor número de delitos posible”, afirma. Lo hacen no sin polémica, dado que se ha demostrado que a menudo estos sistemas perjudican a personas de color, a quienes atribuyen un mayor riesgo criminal (aspecto que abordaremos también en el próximo capítulo, junto con los asociados a la computarización del sistema jurídico y otros mencionados anteriormente).

En logística, el fundador y director ejecutivo de **[Fixr](https://www.fixr.com/), Andrés Torrubia,** un emprendedor en serie experto en *e-commerce*, afirma que la IA está impactando al

---

comercio *online* (y el *offline*) en múltiples frentes. Algunos son hoy muy evidentes y visibles, como mejores recomendaciones de consumo o sobre cosas como cuándo es el mejor momento para comprar productos recurrentes. También en optimización de la logística: desde planificación de rutas de reparto  
a entrega con drones o el uso de robots autónomos en almacenes. Se usa, asimismo, para ayudar a los departamentos creativos a sugerir futuros productos o para generar productos o diseños por sí misma.  
“Y así multitud de ejemplos. Hay muchas cosas en *e-commerce* que hasta ahora se han hecho de forma rudimentaria, algo que está cambiando gracias a la IA”, señala.

Torrubia mencionaba la automatización de la gestión del trabajo en almacén. Es la automatización del trabajo, que se está dando de forma transversal  
en muchas áreas, no solo en la de la industria y la manufactura. En el ámbito del periodismo y los medios de comunicación, sin ir más lejos, medios como The Washington Post, **[Bloomberg](https://innovadores.larazon.es/es/not/inteligencia-artificial-y-periodismo-asi-se-transforma-bloomberg)** y muchos otros usan bots para generar noticias factuales en el ámbito financiero o deportivo o para seguir tendencias. En **[Deutsche Welle](https://innovadores.larazon.es/es/not/deutsche-welle-asi-innova-un-medio-publico),** la corporación mediática pública alemana, han creado gracias a la IA una herramienta de transcripción, traducción y voz de forma automatizada para contenido de vídeo y otras soluciones para transcripción simultánea, distribución de contenido o verificación de hechos.

Sin duda todo esto permite que los procesos sean más eficientes, aunque los editores destacan que la IA nunca podrá sustituir al juicio humano y que estos siempre formarán parte de la cadena de producción de noticias. Además, si las máquinas se encargan de las tareas repetitivas, las personas podrán ocuparse de lo importante, del trabajo de valor añadido. Es la promesa de la automatización: acabar con los trabajos alienantes y servir como herramienta que aumenta las capacidades de los trabajadores. Una promesa que choca con la paralela destrucción de empleo que provoca la sustitución de personas por máquinas. Esta es otra de las cuestiones que abordaremos en el próximo capítulo, sobre el impacto social de la IA y sobre las consideraciones éticas al respecto.

---

######## 02

## Contexto Social de la IA

### **La tecnología nos ha permitido cazar, agruparnos, comunicarnos;** nos ha permitido asentarnos en ciudades, aumentar nuestra esperanza de vida, conectar nuestras ideas... Pero esta también ha alimentado guerras, batallas ideológicas o desigualdades y nos ha llevado al borde del precipicio, a arriesgar a la propia humanidad casi hasta el borde de la extinción, todo por conseguir tecnologías más poderosas. Es, al fin y al cabo, el equilibrio entre tecnología y riesgos.

Así comenzaba su discurso en el Future Trends Forum **Jade Leung,** directora de Investigación y Asociaciones con el Centro para la Gobernanza de la Inteligencia Artificial (**[GovAI](https://www.fhi.ox.ac.uk/govai/)**) en el Instituto para el Futuro de la Humanidad asociado a la Universidad de Oxford. En efecto, ante la implementación y uso de cualquier tecnología se deben considerar sus potenciales beneficios y contrapartidas, y la Inteligencia Artificial no es diferente.

En el anterior capítulo hemos hablado de las aplicaciones actuales de la IA y de cómo esta puede ayudar a mejorar la salud de las personas, la gestión y optimización de negocio, la movilidad, las finanzas, la logística o los servicios legales y jurídicos. Asociados a estos beneficios, emergen también efectos negativos o, cuanto menos, riesgos y retos en su implementación. De ellos hablamos en el siguiente apartado.

---

######## 2.1

##### Impacto social de la Inteligencia Artificial. Riesgos y retos

######## 2.1.1

###### Impacto en el comportamiento humano

Tal y como señala la investigadora en ética **Fiona McEvoy,** fundadora de **[YouTheData.com](https://youthedata.com/),** la gama de desafíos éticos y sociales a la que nos enfrentamos es enorme, al igual que las preocupaciones. Entre ellas cita los sesgos algorítmicos, sistemas injustos o que no respetan la privacidad o la seguridad debidas, o que tratan de influir en nuestras opiniones y decisiones en beneficio de otros, o que fomentan la adicción a estar conectados.

McEvoy sostiene que en el epicentro de todo esto está la asimetría de la información: el desequilibrio entre los usuarios (personas normales y corrientes que viven sus vidas) y las grandes empresas tecnológicas y, en algunos casos, también con respecto a los gobiernos, que son quienes tienen la sartén por el mango en tecnologías basadas en datos e Inteligencia Artificial.

Su ámbito de trabajo es la influencia. La idea de cambiar el comportamiento en base a la IA, a partir de toda la información sobre quiénes somos, qué nos gusta, qué preferencias tenemos o dónde estamos y las predicciones sobre qué nos va a gustar, dónde vamos a estar, qué vamos a comprar, etc. “En algunos casos, las entidades comerciales se aprovechan de nuestros sesgos cognitivos, de la parte irracional de la arquitectura de toma de decisiones, para darnos un empujoncito a comprar sus productos y servicios”, afirma la investigadora.

La razón por la que esto le inquieta es que nos dirigimos a entornos cada vez más inmersivos. Se refiere al uso de asistentes virtuales y a las realidades extendidas (virtual, aumentada y mixta), donde se construyen entornos completos

para el usuario. Para ejemplificar su potencia, cita un **[estudio](https://vhil.stanford.edu/mm/2010/bailenson-ow-virtual-doppelgangers.pdf)** de la Universidad de Stanford de 2009, cuando estas tecnologías eran mucho menos sofisticadas. La investigación mostró cómo un grupo de niños expuesto a una experiencia de realidad virtual donde veían avatares de si mismos nadando con orcas recordaba esa experiencia como real al cabo de un par de semanas.

McEvoy asegura que el uso de estas tecnologías puede aumentar significativamente la influencia que pueden ejercer sobre los usuarios otros entes, en base a ellas y a una personalización mayor a través de la IA. “Todos los datos que ya dominan nuestros hábitos, nuestros clics, se van a sumar en tiempo real a información dinámica sobre biometría [que monitoriza nuestro cuerpo, incluidos movimientos como hacia dónde miran nuestros ojos] o el registro de signos vitales como nuestra frecuencia cardiaca”, dice.

A la manipulación contribuye, sostiene, que estos sistemas se diseñen con características humanas, con apariencia, voz o comportamiento similar al de una persona. Cree que esto responde a una fascinación en la comunidad tecnológica por tratar de replicar a los humanos. “Entiendo que hasta cierto punto sea necesario pero tal vez deberíamos reconsiderar en qué medida”, asegura. Las personas deberían sentir que realmente esa app, asistente, robot o programa con el que están tratando no es más que eso, un dispositivo, y esa es una cuestión de transparencia.

**Dor Skuler,** CEO & Cofundador en **[Intuition Robotics](https://intuitionrobotics.com/),** dice estar de acuerdo en la transparencia y la honestidad de que la tecnología no pretenda parecer humana. “Esta es la razón por la que diseñamos el robot ElliQ, para que sea un ‘objetoide’ en lugar de un ‘humanoide’. ElliQ no tiene forma humana, ni ojos ni manos, y su voz es claramente mecánica”, asegura. Sin embargo, no cree que se deban forzar ciertas reglas de diseño, sino que este debe basarse en las necesidades y objetivos del producto que se está desarrollando.

---

Otra forma en la que la IA puede influir en nuestro comportamiento, según McEvoy, es a través de la idea de sentirnos vigilados en público. “En las calles de San Francisco (EE.UU.), donde vivo, se acaba de prohibir el reconocimiento facial. En Reino Unido, mi país de origen, hay un uso cada vez más agresivo por parte de las autoridades públicas para recopilar datos faciales biométricos muy personales”, afirma. Es lo que la economista y profesora emérita de Harvard **Shoshana Zuboff** describe en su libro *La Era del Capitalismo de la Vigilancia: La Lucha por un Futuro Humano en la Nueva Frontera del Poder* (Public Affairs, 2019). La imagen orwelliana del Gran Hermano que se vale de tecnologías cada vez más sofisticadas para controlar a las personas, ya sea como usuarios o como ciudadanos.

El último argumento de McEvoy sobre cómo se trata de influir en nuestro comportamiento tiene que ver con la autenticidad en dos sentidos. El primero, en relación a nuestra imagen. “Parece que todo el mundo lo sabe todo sobre nosotros, hay más información sobre mí de lo que yo sé sobre mí misma o de lo que puedo recordar. Pero cada vez más, como usuarios, sabemos menos, y se nos engaña de forma habitual”, afirma. El segundo tiene que ver con la desinformación, con las noticias falsas y los llamados *deep fakes*, que emplean sistemas de aprendizaje profundo para generar sofisticadas falsificaciones -en especial de vídeos o imágenes- que no son obvias a simple vista. “Cada vez es más difícil diferenciar entre qué es real y qué es falso, qué es auténtico y qué no”, concluye.

######## 2.1.2

###### Impacto en el empleo

En el capítulo anterior señalábamos cómo la automatización que potencia la IA choca con la paralela destrucción de empleo que provoca la sustitución de personas por máquinas. “Quiero enfatizar que, como haría cualquier cambio tecnológico de esta magnitud, la automatización de la producción y la producción de vehículos autónomos, van a afectar al empleo en el sector automotriz, que es esencial en Europa”, afirma Raúl Rojas, investigador y profesor en la Universidad Libre de Berlín (Alemania), que en el capítulo anterior hablaba sobre su experiencia en el desarrollo de coches autónomos.

Afectará en forma de reducción de vehículos, con su consiguiente impacto en las fuerzas de ventas, y también al empleo de los conductores, ya sea de VTC, de taxis o de camiones. Por ello, Rojas subraya la necesidad de encontrar una solución. “A veces me siento un poco culpable por trabajar en esto, porque la IA es un pacto con el diablo. Por una parte se intenta hacer algo bueno, pero eso causa algo negativo y hay que intentar encontrar un equilibrio”, afirma. “Tenemos que ser conscientes de que lo que hacemos en IA afecta a muchas personas de manera negativa”, añade.

---

Aunque dicha afirmación parece obvia, mucha gente cae en la trampa de negarla, según afirma **Calum Chace,** escritor y ponente experto en IA. “Descartan la idea de la destrucción de empleo como si fuera una falacia”, dice el también fundador del Club de la Singularidad Económica [que define como “un grupo de tecnólogos, académicos y escritores que piensan que vale la pena tomar en serio la amenaza del desempleo tecnológico masivo”]. Chace asegura que los escépticos suelen citar el ejemplo del sector agrícola, que pasó de ocupar al 80% de los trabajadores de Estados Unidos a un 1% actual.

“Fue una transición muy difícil y, para ser francos, murió bastante gente, pero por lo menos no provocó un desempleo tecnológico generalizado de por vida”, ironiza. La automatización está en su mayoría relacionada con la mecanización. Cuando las máquinas reemplazaron trabajos físicos, aún podíamos ofrecer nuestras capacidades cognitivas. Ahora estas también son objeto de automatización y advierten -dice Chace- de lo que nos espera en el futuro.

El autor sostiene que la Ley de Moore [según la cual el número de transistores que caben en un circuito integrado se multiplica cada dos años] está evolucionando. “Eso nos proporciona un magnífico crecimiento de potencia de nuestras máquinas, que se están volviendo mucho más poderosas”, afirma. Según esta regla, dentro de 10 años serán 128 veces más poderosas de lo que son hoy en día, y un millón de veces más dentro de 30 años. Por este motivo, Chace cree que “decir que el desempleo tecnológico no va a ocurrir porque nunca se ha dado el caso, roza la complacencia”.

¿Qué vamos a automatizar? “Rojas nos ha contado que probablemente, para el año 2030, en algunas ciudades no habrá muchos taxistas, y creo que lleva razón”, asegura. Esos taxistas y camioneros -hipotetiza- pasarían a trabajar en almacenes, fábricas o centros de llamadas, que a su vez están cada vez más automatizados. “La automatización no sólo afectará a los empleos obreros baratos y repetitivos, sino a todo tipo de profesiones”, añade el autor.

El informe ***[How robots change the world. What automation really means for jobs and productivity](https://www.oxfordeconomics.com/recent-releases/how-robots-change-the-world?fbclid=IwAR3HEovXJp6xrRBN5vgKjybXCAv1Pls5BdWsDIUnnJd1RzRohQqKpHLV5jE)*** de Oxford Economics (2019) señala cómo la automatización empieza a afectar también al sector servicios. “Las

innovaciones en Inteligencia Artificial, aprendizaje automático y poder de cómputo sugieren que se está produciendo una aceleración significativa en la adopción de robots en dicho sector, particularmente en logística, liderada en parte por la expansión global de Amazon y otras compañías multinacionales de comercio electrónico”, señala el documento. Sin embargo, el estudio destaca que será difícil que las máquinas reemplacen a los humanos en ocupaciones que requieran compasión, creatividad e inteligencia social. Por ejemplo, los fisioterapeutas o los trabajadores sociales.

Por otra parte, el libro **[The Technology Trap Capital, Labor, and Power in the Age of Automation](https://press.princeton.edu/books/hardcover/9780691172798/the-technology-trap)** (Princeton University Press, 2019) echa por tierra una de las estadísticas más citadas de los últimos años en el ámbito de la automatización de trabajo. Se trata del hallazgo de que el 47% de los empleos estadounidenses tienen un alto riesgo de automatización a mediados de la década de 2030, que dos académicos de la Universidad de Oxford publicaron en un artículo en 2013. Ahora, uno de sus autores, **[Carl Benedikt Frey](https://www.carlbenediktfrey.com/),** quiere aclarar el malentendido en su nueva publicación. “No es cierto que la mitad de todos los trabajos se automatizarán en una o dos décadas”, afirma en una entrevista en ***[The Economist](https://www.economist.com/business/2019/06/27/will-a-robot-really-take-your-job).*** Según explica, lo que predecía su estudio es el porcentaje de empleos más vulnerables a la automatización, lo cual no significa que, de facto, se automatizarán. Eso -decía el propio artículo- dependerá de muchas otras cosas, como el coste de la tecnología, las preocupaciones regulatorias, la presión política y la resistencia social.

Frey sostiene que el uso sesgado de los datos publicados en su texto refleja la polarización del debate sobre la naturaleza de la automatización y el futuro del trabajo. Lo que a él le inquieta no es ya la destrucción de empleo, sino su precarización, “que provocará disturbios y oposición, lo que a su vez podría frenar el ritmo de la automatización y el crecimiento de la productividad”. A Frey no le preocupa un mundo con demasiados robots, sino un futuro con muy pocos. Cree que los legisladores deben gestionar la transición con una ampliación del seguro salarial y otras compensaciones, y con una reforma educativa, entre otras medidas.

En línea con la desmitificación de la destrucción del empleo está lo que la antropóloga e investigadora en Microsoft Research **[Mary L. Gray](https://marylgray.org/)** denomina “la paradoja de la última milla de la automatización”: que el deseo de eliminar el trabajo humano siempre genera nuevas tareas para los humanos. De ello habla en su reciente libro ***[Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass](https://ghostwork.info/)*** (Houghton Mifflin, 2019), en el que ella y el informático social Siddharth Suri examinan el impacto de la automatización a través de las experiencias de los trabajadores de la economía *online* bajo demanda.

---

Los investigadores se refieren a lo que se conoce como ‘computación humana’: trabajos que empiezan y terminan *online* y que realizan cualquier tipo de tarea que pueda ser administrada, procesada, efectuada y pagada en línea. Por ejemplo, las que se solicitan a través de plataformas como Amazon Mechanical Turk. Solo las tareas de etiquetado relacionadas con la IA supondrán un mercado global de más de 1.000 millones de dólares a finales de 2023, según un informe de Cognilytica.

Estos trabajos -etiquetado, clasificación, identificación de discursos de odio, etc.- potencian los sistemas, sitios web y aplicaciones de IA que todos usamos y damos por sentado. TripAdvisor, Match.com, Google, Twitter, Facebook o la propia Microsoft son algunas de las empresas más conocidas que generan tareas bajo demanda en estas plataformas. “Cada día surgen nuevas compañías con modelos de negocio que dependen de ellas. Este tipo de trabajo no solo está aumentando, sino que se traduce, de facto, en una reorganización más amplia y profunda del empleo en sí”, aseguran los autores. El problema de esta forma de trabajo -dicen- es que podría hacer invisible la labor de cientos de millones de personas. De ahí que le llamen ‘trabajo fantasma’.

Gray, como Frey, señala que parte de la solución a esto pasa por aumentar las coberturas sociales de los trabajadores, pero cita otras muchas medidas a considerar. Sobre estas y otras posibles soluciones hablaremos en el siguiente apartado.

#### La solución al empleo precarizado pasa por aumentar las coberturas sociales de los trabajadores, entre otras medidas

######## 2.1.3

###### Impacto en la justicia

Ya en el capítulo anterior **Christopher Markou** avanzaba cómo está impactando la IA en el sistema jurídico: los sistemas de incentivos, la burocracia y las tareas de gestión y de administración y hasta intentos de usar robots para sustituir a jueces y abogados. ¿Cuáles son los límites en la informatización de la ley? Si la “ley” no es más que un

elaborado sistema de reglas, quizás respaldado por una sanción o amenaza, tal vez gran parte de ella sea, por lo menos en cierto sentido, computable, dice el profesor e investigador de la Universidad de Cambridge.

Markou cree que esa es una manera incorrecta de concebirlo. “El derecho es ante todo una institución social, con normas, categorías y marcos socialmente construidos para comprender el mundo”, afirma. Estos han evolucionado durante largos períodos de tiempo y tratar de formalizarlos con métodos matemáticos o lógicos corre -en su opinión- el riesgo de simplificar en exceso el mundo para adaptarlo a un modelo computable. También de degradar una de sus funciones sociales y antropológicas -a su juicio- más importantes: proteger a la sociedad y al individuo de los efectos dañinos y “potencialmente deshumanizadores” de la tecnología.

Codificar las leyes y el conocimiento legal en un sistema legible por máquina resulta además muy difícil, según el experto. Entre otras cosas porque la prestación de asesoramiento legal o adjudicación legal no se produce en un contexto cerrado. “Un abogado competente necesita entender e intuir por qué algo como un caracol en una botella de cerveza de jengibre es algo malo. Es natural que la inteligencia humana haga este tipo de inferencias complejas a partir de hechos, pero es excepcionalmente difícil para un sistema informático, incluso para los más avanzados”, explica Markou.

Otro obstáculo en la informatización de las leyes que destaca el investigador es que, para desarrollar algoritmos de aprendizaje automático se debe establecer una ‘función objetivo’, una medida empírica de éxito o precisión en cualquier tarea que tenga en mente, algo que no es posible en decisiones legales en las que las concepciones de lo que es ‘bueno’ o incluso ‘óptimo’ varían según el contexto cultural, los cambios a lo largo del tiempo y los distintos clientes. Además, muchas preguntas legales -dice- requieren una simulación muy precisa de la mente humana. Por ejemplo, el concepto de ‘persona razonable’ requiere

---

que un abogado interprete un contrato al crear una construcción mental de cómo esta persona ficticia actuaría. “No es suficiente que una computadora emule aspectos de la inteligencia humana, debe ser capaz de pensar como un humano”.

#### Codificar las leyes y el conocimiento legal en un sistema legible por máquina resulta muy difícil

Por último, Markou destaca que la ley es, en última instancia, el lenguaje. Y este es “tremendamente difícil de descifrar y entender para una computadora, ya que es increíblemente impreciso”. Uno de los ejemplos que cita es el problema del conocimiento del sentido común: saber cosas y generalizarlas a nuevos escenarios o resolver problemas cuando tenemos información incompleta. “Si bien la IA está mejorando en este tipo de tareas, aún está muy lejos del tipo de inteligencia que posee un niño promedio”, afirma el experto.

Por estos motivos, el profesor no ve con buenos ojos la iniciativa estonia de llevar ‘robojueces’ a la práctica. “El verdadero peligro no es que estos sistemas funcionen, lo cual sería genial. El problema es que no lo hagan, pero creamos que sí y los utilicemos bajo esa errónea asunción” declaraba Markou en una entrevista en ***[Innovadores](https://innovadores.larazon.es/es/not/preparados-para-los-robojueces).*** Sostiene que, si bien hay razones para creer que la Inteligencia Artificial puede ayudar en casos relativamente poco sofisticados, la pregunta crítica es en qué otros contextos legales se debe permitir que los jueces algorítmicos tomen decisiones.

La filósofa **[Lorena Jaume-Palasí](https://www.linkedin.com/in/lorenajaumepalasi/),** fundadora de **[Ethical Tech Society](https://www.ethicaltech-society.org/),** coincide en buena parte del análisis de Markou. “La IA no puede contextualizar. Por ello la interpretación de la ley, que es el trabajo de un juez y una tarea de contextualización, no lo puede desempeñar de forma automatizada un programa de software”, asegura. Tampoco cree que sea posible en un futuro cercano, y menos aún deseable. “Creo que esa es uno de las tareas dentro del trabajo judicial que jamás debería automatizarse”, afirma.

Jaume-Palasí apunta, no obstante, que estos sistemas pueden ayudar a detectar inconsistencias en las decisiones de un juez, ya que “factores externos como el tiempo, haber comido o la hora del día influyen en la toma de decisiones”.

La experta insiste, no obstante, en que la parte esencial de lo que constituye ser juez (ponderar la ley, buscar factores agravantes o desagravantes, comprobar si el veredicto contradice las intuiciones éticas de la sociedad, etc.) no puede ser automatizado.

El empeño en hacerlo es lo que le parece a la filósofa más preocupante: ¿se trata de hacer la justicia más eficiente y justa o de crear máquinas capaces de sustituir al ser humano por el mero hecho de que es posible hacerlo? Markou opina lo mismo: “Que tengamos tecnologías capaces de automatizar la justicia no significa que debamos hacerlo, como muestra la decisión de usar bombas atómicas en la Segunda Guerra Mundial. Debemos primero cuestionar las consecuencias de su uso, atendiendo a los principios de prudencia y previsión”, afirma.

En este sentido, los valores de eficiencia y velocidad asociados a la tecnología no son los prioritarios para el sistema legal. Sí lo es la justicia -dice Markou- si bien es ideal que esta sea rápida. Como escriben el filósofo Evan Selinger y el periodista Clive Thompson en ***[El engaño de la eficiencia](https://onezero.medium.com/the-efficiency-delusion-f6a97241e1e1),*** esta no siempre es un valor neutral. Ponerla por encima de otros valores puede ser un error, un lapso en el juicio ético, político, personal o profesional. Algunas interacciones humanas o cívicas prosperan cuando son deliberadas y se erosionan cuando se aceleran.

De forma acelerada o no, que la ley sufrirá transformaciones sustanciales es algo que Markou no niega: “No hay ninguna razón para excluir categóricamente a la tecnología como elemento en su mejora. Sin embargo, los gobiernos y el público deben ser críticos y escépticos acerca de la naturaleza, el grado y el ritmo de esta transformación. Tener mecanismos robustos,

---

efectivos y deliberativos para ayudar a guiar el desarrollo de la tecnología y democratizarla en beneficio de todos”, concluye el investigador.

######## 2.1.4

###### Impacto en la gobernanza

**Jade Leung** nos introducía en este capítulo desde su perspectiva de trabajo en gobernanza. “Tenemos una visión muy a largo plazo de lo que significa esa gobernanza y del tipo de consecuencias en el que nos tenemos que centrar”, afirma. El trabajo de la también doctoranda en geopolítica de la IA en la Universidad de Oxford se centra en cómo modelar las relaciones entre las empresas, el gobierno y la comunidad de investigación, en identificar qué dinámicas originan la cooperación y el conflicto.

Leung sienta las bases de su discurso con una reflexión: si echamos la vista atrás y volvemos al punto de partida de cualquier tecnología que se nos ocurra, es obvio que la idea de que podemos predecir y anticipar cómo debería ser la gobernanza tecnológica al inicio de dicha trayectoria es completamente errónea. “A veces somos lo suficientemente prudentes y tenemos la buena suerte de ir por un camino que beneficia a la sociedad, pero a veces no. A veces nos dejamos llevar por el poder de las armas nucleares, a veces somos irresponsables con la ingeniería genética y a veces desplegamos tecnologías sin saber a ciencia cierta qué implicaciones tendrán. Y, en ese momento, tenemos una tecnología que nos provoca mucha incertidumbre, y eso es un riesgo laboral para la gobernanza de las tecnologías de hoy en día”, afirma.

Estos retos de gobernanza son un esfuerzo de navegación -dice- entre las montañas de oportunidades con recursos limitados para evitar caer en los valles de riesgos. La IA es uno de los retos de navegación actuales y, a su juicio, probablemente uno de los más complicados, por dos razones principales. En primer lugar, porque es una

tecnología de utilidad general (GPT, por sus siglas en inglés). “Este tipo de tecnología transforma las funciones esenciales de la civilización, como por ejemplo la producción de energía o la comunicación o, en el caso de la IA, el procesamiento inteligente”, afirma.

#### La IA es una tecnología de utilidad general

Las GPT llevan asociados unos fuertes incentivos de proliferación rápida de las funciones que tienen por toda la sociedad. “Es emocionante ver su potencial económico, pero el motivo de que la gobernanza sea más complicada es que, por naturaleza, no hay un control centralizado ni unos objetivos de gobernanza centralizados, lo cual lo complica”, asegura la experta. “No hemos tenido buenos puntos de referencia gestionando este tipo de tecnologías y hemos cometido muchos errores por el camino”, añade.

---

El segundo motivo por el que - a juicio de Leung- esta tecnología es difícil de regular es porque estamos atrapados en un entorno de competición estratégica en el sector. Hemos pasado de la carrera espacial a la carrera de la IA, con dos claros contrincantes en los primeros puestos: EE.UU. y China. Estas dos potencias están inyectando mucho capital político y financiero para ser líderes en el campo, algo que se da no solo desde el sector público, sino también desde el privado. Siete de las diez corporaciones más grandes del mundo por valor de mercado -según datos del informe 2019 ***[BrandZ Top Global Brands](http://www.millwardbrown.com/brandz/rankings-and-reports/top-global-brands)*** de Kantar- son empresas que tienen la IA en su corazón (Amazon, Apple, Alphabet, Microsoft, Facebook, Alibaba y Tencent), que invierten enormes sumas en I+D. “Tanto en el dominio público como en el privado hay un fuerte aliciente de competición, lo cual no tiene por qué ser malo, pero lo es cuando esta exacerba los riesgos de tipo estructural”, comenta la experta.

Estos no son los únicos peligros. Hay otros incluso mayores. Estas compañías tienen un inmenso poder económico, de penetración y tecnológico y, con él, de forma indirecta, también de gobernanza. Todas ellas -salvo Tencent- se han unido en un consorcio denominado **[Partnership on AI](https://www.partnershiponai.org/)** al que se acusa de actuar como grupo de cabildeo a la búsqueda de normas -o ausencia de ellas- que beneficien a sus intereses. Una forma de autorregularse sin someterse al escrutinio y voto público. “La autorregulación no es una opción. No es posible que los gobiernos y los ciudadanos confíen en estas empresas solo porque digan que se comprometen a ser éticas, responsables o cualquier otra palabra de moda que sea conveniente para sus negocios”, comenta Markou.

Estas compañías tienen derecho -y es su deber- a participar en las discusiones sobre cómo fomentar la innovación o luchar contra el discurso del odio, contra la manipulación o el pirateo. Pero la línea entre tener voz en ellas y dictar, de facto, las leyes, se hace cada vez más fina. Sobre ello escribe en ***[Bloomberg](https://www.bloomberg.com/opinion/articles/2019-01-17/beware-of-tech-companies-playing-government)*** la europarlamentaria progresista del D66, Marietje Schaake: “Está muy bien que las empresas actúen de manera responsable, pero otra cosa diferente es que asuman la gobernanza transfronteriza y la responsabilidad de crear normas a nivel mundial, lo cual es profundamente problemático para la democracia y para el Estado de derecho”, afirma.

Schaake alerta del riesgo de que el sector privado capte el interés público y de que las normas se hagan sin transparencia, sin responsabilidad y sin el mandato de las personas. Es lo que llama “la privatización de la gobernanza”. “Los imperios tecnológicos están dictando decisiones sobre nuestra vida e imponiendo sus leyes. Quieren conectarnos a todos para controlarnos a todos”, asegura en un artículo en ***[El País Retina](https://retina.elpais.com/retina/2019/04/12/tendencias/1555058434_379325.html)*** Renata Ávila, abogada de Derechos Humanos y directora ejecutiva de la Fundación Ciudadanía Inteligente.

#### La privatización de la gobernanza es el riesgo de que el sector privado capte el interés público

Otra consecuencia, en un supuesto futuro en el que la IA se haya totalizado en la sociedad y los jueces y abogados hayan sido reemplazados por sistemas computacionales, es la de reemplazar el ‘Estado de derecho’ por un ‘Estado de tecnología’, en el que los sistemas que administran la justicia son opacos e ininteligibles para todos menos para quienes los controlan, hipotetiza Markou. En su ensayo **[Fabricando la revolución de la inteligencia artificial](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3078224),** el biólogo de sistemas Yarden Katz, afiliado a la Escuela Médica y al Centro para internet y la sociedad Berkman Klein de la Universidad de Harvard, habla sobre la falsa impresión de que los sistemas actuales han superado las capacidades humanas hasta el punto de pensar que las máquinas pueden manejar mejor muchas áreas de la vida. Estas afirmaciones -dice el investigador- se basan en una visión limitada y radicalmente empírica de la inteligencia humana. Es lo que el erudito francés Alain Supiot llama “gobernar por números”: una manera de limitar el pensamiento acumulando métricas sobre métricas.

‘Los números no mienten’ es una suposición peligrosa, dice **David Weinberger,** filósofo, tecnólogo e investigador cuyo perfil introducíamos en el capítulo anterior. El aprendizaje automático -prosigue- aprende de los datos que le damos, y estos a menudo reflejan prejuicios existentes. Además, decidimos qué datos creemos que son relevantes para un problema, otra forma en que la subjetividad humana y el sesgo dan forma a la IA. También decidimos qué tipo de resultados nos parecen aceptables y lo que creemos que constituye equidad.

Weinberger destaca que, si bien utilizamos estos sistemas cuando pensamos que sus resultados son superiores a los que puede obtener un humano, a veces dicha superioridad es simplemente una cuestión de mayor velocidad o menor precio. “En los casos en los que usamos la IA por ser más precisa -dice- podemos decir que son una forma superior de toma de decisiones, asumiendo que hemos decidido de manera autónoma actuar sobre sus resultados. Pero eso no los hace supremamente racionales excepto en una idea limitada de racionalidad”, sostiene.

Los sistemas de aprendizaje automático encuentran patrones estadísticamente relevantes y/o útiles mediante correlaciones, algunas de las cuales pueden ser directas y causales, y otras pueden ser señales y

---

signos. Eso los hace mejores para hacer su trabajo, asegura el experto. Al contrario que Markou, no cree que ello nos deshumanice; al menos no más que usar ordenadores para calcular o usar libros para recordar. “Creo que pone a los humanos en una perspectiva diferente y más realista. Nos recuerda que nuestras mentes son bastante limitadas en cuanto a la cantidad de información que pueden gestionar. Darnos cuenta de ello nos humaniza. Lo que es deshumanizante es pensar que somos el epítome  
de la racionalidad mortal destinada a comprender el orden del universo en sus detalles, lo cual no creo que sea cierto”, añade. Markou está de acuerdo en que la toma de decisiones humanas tiene muchos sesgos y deficiencias. La solución -dice- no debe ser eliminarnos del proceso, sino tratar de mejorar su naturaleza y calidad.

#### Los sistemas de aprendizaje automático encuentran patrones estadísticamente relevantes y/o útiles mediante correlaciones

######## 2.1.5

###### Impacto en la privacidad

Entras a una tienda y te conectas al WiFi. En la página 15 del décimo acuerdo de términos y condiciones que tienes que aceptar, hay una pequeña cláusula que dice “si te conectas a nuestra red, podemos rastrear a dónde vas, te podemos seguir incluso cuando salgas de la tienda”. Sin leerlo, haces clic en “aceptar” y la empresa empieza a rastrearte. Sabe que vas al gimnasio, comes fuera, a qué médicos visitas en el hospital, lo rápido que te duermes y cuánto. Ahora la tienda tiene muchísimos datos de tu estilo de vida, que usará, como mínimo, para venderte cosas.

Es el ejemplo que pone **Adam West,** director de Marketing de Satalia, que en el anterior capítulo nos hablaba de  
las aplicaciones de la IA en marketing y publicidad. No es un caso real, pero podría ser uno de los muchos que se dan a diario. Los escándalos son constantes, como investigaciones que revelan el uso ilegítimo de datos, violaciones de intimidad o abusos contra la privacidad y grandes multas a las dos grandes tecnológicas en el punto de mira: Google y Facebook.

Entre los escándalos más recientes: más de 1.000 aplicaciones de Android (el sistema operativo móvil de Google) recopilan datos incluso después de que los usuarios les nieguen los permisos para ello, usando formas de eludir el sistema de seguridad. Como en el ejemplo de West, se aprovechaban, entre otras cosas, de localizaciones de puntos WiFi. Lo ha revelado un **[estudio](https://www.ftc.gov/system/files/documents/public_events/1415032/privacycon2019_serge_egelman.pdf)** con repercusiones mundiales realizado por investigadores de la Universidad Carlos III de Madrid, de la red IMDEA y de la Universidad de Berkeley (EE.UU).

Varias organizaciones en varios países europeos han interpuesto recientemente denuncias en sus respectivas agencias de protección de datos contra la industria de la publicidad *online*, especialmente contra Google. Les acusan de “fuga masiva de datos” y de no cumplir, con ello, el Reglamento General de Protección de Datos (RGPD). Denuncian específicamente que la industria sigue utilizando el sistema de subasta en tiempo real para compartir datos personales con terceros, sin conocimiento ni control por parte de los usuarios.

Tal y como explica un artículo en **[El País Retina](https://retina.elpais.com/retina/2019/05/28/tendencias/1559040361_176907.html),** el sistema funciona de forma que, al entrar en cualquier web, los datos de nuestro perfil *online* van a parar a un servidor de anuncios. Después llega a un sistema de oferta de espacios publicitarios e impresiones. Y luego a otro de intercambio de anuncios (que conecta los datos de las diferentes demandas que lanzan los anunciantes con los de los espacios publicitarios). Finalmente, una de esas demandas de anuncio segmentado será escogida y aparecerá en la próxima web visitada, que obtendrá nuevos datos para completar la información que ya tiene sobre nosotros.

¿Qué información se envía? Tal y como explica uno de los impulsores de la denuncia -el director de Política y Relaciones con la Industria del navegador Brave, Johnny Ryan- los datos de cada usuario que reciben esos miles de terceros son:

- Nuestro historial de búsqueda. Lo que vemos, miramos o escuchamos.
- Categorías de contenido: cualquier cosa que hayamos buscado por cualquier motivo y por sensible o íntima que sea se asocia a nuestra persona bajo categorías como incesto, abuso de drogas, infertilidad, salud mental, ideología política…
- Nuestro identificador (ID), pseudónimo único, que oculta nuestra identidad pero permite que se nos reconozca bajo ese ID en siguientes visitas.
- Dicho ID asociado al perfil de nosotros que tienen los compradores de anuncios.
- Nuestra geolocalización.
- Una descripción de nuestro dispositivo.

---

- Nuestra dirección IP (en algunos casos, dependiendo del sistema de pujas en tiempo real usado).

Ryan destaca la posibilidad y alta probabilidad de que los diferentes actores implicados en el proceso de subasta conecten toda la información de la que disponen sobre ti para tener un perfil más completo. Una de las consecuencias es que cada persona *online* puede ser ampliamente perfilada. “Solo con la dirección IP ya tienen un identificador único que pertenece a tu dispositivo y que, por tanto, pueden asociar a tu nombre y apellido”, señala Gemma Galdon, presidenta de la **[Fundación Éticas](https://eticasfoundation.org/)** dedicada a la realización de estudios y actividades de concienciación sobre cómo impactan las nuevas tecnologías en la sociedad.

La Interactive Advertising Bureau (IAB) reconoce no haberse adaptado aún al RGPD, pero critica que dichas denuncias “no distinguen los actores que cumplen de los que no, y meten a todo tipo de empresas en el mismo saco”. Creen que la visión ofrecida por las organizaciones demandantes “no representa a todo un sector”.

Entre los que parecían salvarse está Apple, que ha hecho de la privacidad su bandera. Recientemente conocíamos el descontrol de la compañía de Cupertino sobre la privacidad de las aplicaciones en la Apple Store. Una investigación del columnista de tecnología Geoffrey A. Fowler junto con la empresa de privacidad **[Disconnect](https://disconnect.me/)** publicada en ***[The Washington Post](https://www.washingtonpost.com/gdpr-consent/?destination=%2ftechnology%2f2019%2f05%2f28%2fits-middle-night-do-you-know-who-your-iphone-is-talking%2f%3futm_term%3d.4ddb2ce65299)*** revela que había 5.400 empresas rastreando su iPhone por semana, compartiendo con terceros 1,5 gigabytes de datos en solo un mes y que, entre los rastreadores que estaban enviando su información, se encuentran aplicaciones como Spotify, Microsoft OneDrive, Nike, Weather Channel o la propia app de The Washington Post.

La App Store -comenta Fowler en su artículo- requiere que los desarrolladores tengan políticas de privacidad claramente publicadas, pero estas políticas no necesariamente proporcionan protección. También requiere que las aplicaciones soliciten permiso a los usuarios para recopilar datos antes de hacerlo. Muchas -dice- no lo hacen, ni revelan los nombres de las compañías que les rastrean o cómo protegen los datos personales. Cuantos más puntos de transferencia de datos personales haya, más difícil será responsabilizar a las empresas por mal comportamiento.

Estos no son, obviamente, los únicos modos en los que se pueden producir fugas de datos. Además de los móviles, hay otros muchos dispositivos que hoy en día están conectados y pueden compartir su información con terceros. Hablamos del Internet de las Cosas (IoT). Según el informe ***[Cisco Visual Networking Index](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/white-paper-c11-738429.pdf)*** (VNI), para 2022 habrá 3.9 billones de conexiones de IoT móviles, cuatro veces más que en 2017. El informe ***[Ericsson Mobility Report](https://www.ericsson.com/en/mobility-report/reports/november-2018)*** cifra estas conexiones en 4,1 billones para 2024.

A medida que aumenta el número de dispositivos conectados, aumenta el riesgo de ciberataque. Su uso a nivel industrial, en el hogar conectado o en nuestro cuerpo (mediante *wearables*) supone una amenaza. La amplia variedad de dispositivos que se puede conectar (televisores, termostatos, cerraduras, alarmas...) crea una gran cantidad de puntos de acceso posibles para piratas informáticos, que podrían incluso penetrar en las tripas de los sistemas de los fabricantes.

La científica de datos **Lauren Dyson** advierte sobre ello: “veo el mayor potencial, y también los mayores riesgos, en la intersección de *big data* y la optimización impulsada por la IA, habilitada por el IoT”, afirma. Cita el caso de uso de estos sistemas en la infraestructura urbana mediante, por ejemplo, la incorporación de sensores. Esto hace que sea posible recopilar grandes cantidades de datos en tiempo real que pueden informar todo, desde la optimización dinámica del flujo de tráfico hasta cuál de los anuncios se muestran en el tránsito público. Como contrapartida, genera -dice Dyson- importantes preocupaciones sobre la seguridad y la privacidad de los datos, en particular en lo que se refiere a información de nivel individual y de identificación personal. “Necesitamos una supervisión externa sólida para garantizar la administración responsable de estos datos”, reclama.

---

######## 2.1.6

###### Retos técnicos

En el capítulo anterior, **Gary Ang,** director de Estrategia en **[Temasek](https://www.temasek.com.sg/en/index.html),** hablaba sobre cómo las presiones sociales y regulatorias sobre el uso ético de los datos y la IA desempeñan -y lo harán aún más- un papel fundamental en la competencia tecnológica entre países, ya que el acceso a datos es clave para el desarrollo de estas tecnologías. También hablaba de la confianza como activo esencial para que los usuarios compartan sus datos. Ambas, uso ético de los datos y confianza, están directamente relacionadas. Si la primera no se da, la segunda tampoco.

Lo tiene claro **Oliver Smith,** director de Estrategia de Alpha Health, que explicaba anteriormente lo que tratan de lograr con su asistente personal de salud. Precisamente esta es un área delicada por lo íntimo de este tipo de datos. ¿Cuáles son las principales preocupaciones y desafíos éticos en el desarrollo de una aplicación de este tipo? Existen dos desafíos principales, explica. El primero es tal vez el más obvio: para poder ofrecer un asistente personal de salud altamente efectivo necesitan acceder a una gran cantidad de datos del usuario. “No creemos que nadie deba hacer esto a menos que seamos capaces de construir una relación de confianza con ellos, y operar éticamente es una parte importante de ello”, afirma.

El segundo reto está relacionado con el hecho de que realicen recomendaciones y proporcionen herramientas que permitan conocer qué es ‘bueno’ o ‘malo’ para una persona en un determinado momento. “Cuando lo hacemos, a menudo emitimos un juicio sobre para quién consideramos que algo es bueno: para ti hoy o para ti dentro de 10 años. Estos juicios significan que debemos estar atentos a nuestros valores”, afirma.

Smith está convencido de que la ética tiene que existir en el ámbito sanitario para poder tener éxito, entre otras cosas porque es un ámbito en el que las personas tenemos diferentes objetivos. “Hay que poder entender cuáles son los objetivos correctos y en qué momento. Por ejemplo, saber cuándo no debes tomarte otra copa o cuándo puedes decir ‘Has tenido un día horrible, sal a tomarte una copa con tus amigos, te va a sentar bien’. Entender eso es básicamente una decisión ética y moral. Pero hay otro componente ético, que es que te perciban como una persona ética y en la que se puede confiar, y ganarse esa confianza”, afirma.

Para enfrentar estos desafíos, Alpha Health ha desarrollado -cuenta Smith- un conjunto de promesas éticas que definen sus valores. “Nos gustaría poder contribuir para ayudar a otros que se están embarcando en este importante trabajo, pero igualmente no queremos adelantarnos y hacer

reclamos antes de que hayamos probado que podemos cumplirlos”, comenta el directivo. Por el momento, están probando cómo pueden implementar su visión ética tanto en sus procesos como en sus productos. Por una parte, han empezado a introducir auditorías externas de su trabajo “para comprender mejor lo que necesitamos mejorar”, señala. Por otra, dedican sus esfuerzos a tratar de averiguar qué plantillas de diseño funcionan mejor, cómo lograr que las políticas de privacidad sean más comprensibles, o cómo desarrollar algoritmos intrínsecamente explicables.

Este, el de la explicabilidad, es uno de los meollos del asunto. Es, para Smith, uno de los desafíos menos entendidos en relación con la obtención de beneficios de la IA en general, y en el marco de la salud y de la medicina precisamente porque la opacidad de sus algoritmos más avanzados la convierten en un anatema para la regulación médica. “Las agencias reguladoras requieren, con razón, que los medicamentos y dispositivos produzcan resultados consistentes en el mismo conjunto de circunstancias, y que cualquier desviación de estos resultados pueda explicarse y entenderse. Sin embargo, esto es imposible de hacer con las técnicas de aprendizaje automático más avanzadas, por ejemplo, el aprendizaje profundo”, explica.

En tales sistemas, el algoritmo es esencialmente una caja negra, ningún ser humano puede explicar por qué da un resultado particular y por qué puede diferir de un conjunto de circunstancias a otro. Si bien esta falta de explicabilidad es inaceptable para los reguladores, profesionales sanitarios y pacientes, la consecuencia -sostiene- es que la medicina no podrá beneficiarse completamente de la IA. Afortunadamente, ya se están realizando esfuerzos -en Alpha Health, entre otras- para solucionar esto mediante la creación de métodos para explicar los algoritmos de aprendizaje profundo. Estos esfuerzos se encuentran en

---

su infancia relativa y su desarrollo debe involucrar a los actores implicados para garantizar el grado adecuado de explicación. También debe ser alentado y apoyado por los reguladores.

######## 2.2

##### Potenciales soluciones

En el apartado anterior hemos visto algunos de los efectos negativos, retos y riesgos que plantean el desarrollo  
y aplicación de la IA y sus tecnologías asociadas. A continuación hablamos de posibles vías para atajarlos o afrontarlos.

######## 2.2.1

###### Marco ético

Los posibles usos ilícitos de la IA han hecho florecer innumerables iniciativas, declaraciones, manifiestos, principios, guías, análisis, etc., enfocados tanto en observar, medir y ponderar cómo afectan estos sistemas a las personas para calibrar posibles efectos negativos, como en el desarrollo preventivo de una IA ética.

Instituciones, gobiernos, compañías de tecnología y consultoras y todo tipo de organizaciones han aprovechado esta oportunidad. Gigantes tecnológicos como Google o Facebook han creado sus códigos éticos y principios. Microsoft ha creado el Instituto de Investigación AI Now “para garantizar que los sistemas de IA sean sensibles y respondan a los complejos dominios sociales en los que se aplican”. Gobiernos como el de Reino Unido han **[declarado su intención](https://www.parliament.uk/business/committees/committees-a-z/lords-select/ai-committee/news-parliament-2017/ai-report-published/)** de abanderar la causa.

A mayor escala, la Comisión Europea tiene varias  
iniciativas relacionadas. Recientemente, este organismo ha publicado varios documentos al respecto. Entre ellos, sus **[Directrices éticas para una IA fiable](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai),** que pretenden ofrecer orientaciones sobre el fomento y la garantía de una IA “ética y robusta”. El documento parte de un enfoque basado en los derechos fundamentales para identificar los principios éticos y sus valores conexos que deben respetarse en el desarrollo, despliegue y utilización de los sistemas de IA. Entre ellos: respeto de la autonomía humana, prevención del daño, equidad y explicabilidad; atención especial a las situaciones que afecten a los grupos más vulnerables así como a las situaciones caracterizadas por asimetrías de poder o de información, reconocer posibles riesgos y adoptar medidas adecuadas para mitigarlos.

El documento de la CE ofrece una serie de recomendaciones para llevar esto a cabo y también indicaciones para evaluar

la fiabilidad de sistemas basados en IA. Si bien esto puede servir de orientación inicial, llevar a la práctica en cada caso específico los códigos y guías éticas es complicado y a veces es excusa para su no implementación. El problema asociado a esto es que los principios de desarrollo ético de la IA se queden en una mera declaración de intenciones. “Para muchas empresas, los esfuerzos públicos para abordar las inquietudes éticas son meros escaparates. Si bien hay un excelente impulso para desarrollar principios y directrices, estas carecerán de significado si las empresas no pueden señalar el trabajo relevante que están realizando para respaldarlas”, afirma McEvoy.

La investigadora es consciente de que todos somos bastante nuevos en esta era de desarrollo tecnológico acelerado en la que “nunca hemos tenido que contemplar las consecuencias morales con tanta urgencia”. La forma más fácil de hacer esto -dice- es saldarlo con declaraciones públicas y promesas que favorecen a su imagen. Markou llama a esto el ‘teatro de ética’ de la industria de la IA: un intento de lavado de cara mediante la invocación de la ética que ya se conoce como *ethics-washing*. A Google de momento le ha pasado una mala jugada, con la polémica cancelación este 2019 de su Comité de Ética -tras conocerse la identidad de algunos controvertidos miembros- una semana después de crearlo.

Christopher Markou cree que el marco de la ética es una especie de rompecabezas. No es suficiente -dice- con tener un equipo que piensa en estas cosas para cumplir con la diligencia debida. Se debe traducir en pasos concretos y en rendición de cuentas. “Si haces algo mal debe tener consecuencias”, afirma. En opinión de Smith, este debate es importante porque conduce a un nuevo tipo de relación, de contrato y de derechos, como cuando los movimientos obreros en el siglo XIX forzaron a mejorar las condiciones de trabajo con normas y límites que además tuvieron como consecuencia un aumento de la productividad.

Para **Weinberger,** la pregunta más interesante es cómo la IA puede estar cambiando algunas de nuestras ideas sobre la moralidad. “La necesidad de proporcionar direcciones extremadamente explícitas a estas máquinas nos lleva a formular de forma expresa múltiples modelos de imparcialidad”, afirma. El filósofo sostiene que las conversaciones sobre estos tipos de imparcialidad y qué tipo de resultados deseamos de las aplicaciones de IA nos están llevando a hablar menos sobre los principios morales y más sobre los valores. **Unos valores que a menudo son inconsistentes entre sí,** por lo que requieren decisiones sobre compensaciones. “Hemos tendido a caracterizar las discusiones sobre las compensaciones de valor como políticas, en lugar de como moralidad. La IA puede llevarnos a pensar que la moralidad es esencialmente política en ese sentido”, asegura.

---

Entre sus consideraciones para un desarrollo ético de la IA cita esencialmente cinco:

1. Afrontar los sesgos, “el pecado original de la IA”, dado que esta aprende de los datos y que esos datos casi inevitablemente reflejan nuestros sesgos existentes. El experto señala que se está haciendo un trabajo muy serio para encontrar formas de identificarlos y minimizarlos, pero es probable que, aun así, este riesgo siga presente en un grado u otro, siempre y cuando la sociedad humana esté sesgada.
2. Las personas en el centro. “Es cada vez más ampliamente reconocido en las comunidades de desarrollo de estas tecnologías que los diversos grupos de personas afectadas por ella deben participar durante todo el proceso. Sus voces, directa e indirectamente, necesitan ser escuchadas”, afirma.
3. Efectos secundarios y terciarios de estos sistemas. “Pueden funcionar perfectamente de acuerdo con nuestras especificaciones y, aun así, tener efectos perversos en una comunidad”, comenta Weinberger. Por ejemplo, el sistema podría tener éxito en la tarea de hacer que las rutas de los autobuses sean más eficientes, pero eso podría hacer que evitase el extrarradio o las zonas más marginales durante las horas punta.
4. Transparencia. “No debemos asumir que la transparencia es la solución correcta para todos los problemas morales planteados por la IA. Y, en caso de que lo sea, debemos ser específicos sobre lo que debe hacerse transparente”, asegura el filósofo. ¿Los datos? ¿Los procesos por los cuales los datos son inspeccionados, limpiados o certificados? ¿Que un grupo diverso de ciudadanos ha estado involucrado en el proceso? ¿Cuáles son los resultados deseados? ¿Cuál es el rendimiento del sistema con respecto a esos resultados? ¿Hay procesos de resolución de problemas en marcha?
5. Debate público. En opinión de **Weinberger,** la elección de los resultados deseados en relación con muchos sistemas de IA debería estar sujeta a debate, decisión y supervisión pública. ¿O acaso queremos que los fabricantes de vehículos autónomos sean libres de decidir a qué valores quieren dar prioridad?

En línea con la visión del filósofo, **McEvoy** resalta la importancia de considerar qué tipo de tareas deben ser automáticas y necesitan la IA y cuáles deben llevar a cabo los humanos. También habla, como Weinberger, de adecuar los productos a los intereses de las personas. Cree que consultar con una amplia y diversa gama de partes interesadas y fuentes para asegurar que se abordan sus preocupaciones más apremiantes debe ser una prioridad de diseño y de negocio.

La investigadora insiste en que el uso de la IA “debe abrirse a un debate público en toda regla”, anterior al propio

desarrollo de la tecnología. “Es imprescindible que, como sociedad, tengamos la oportunidad y seamos capaces de evaluar y revaluar los parámetros de la acción moral de la IA. A veces habrá casos obvios de mal uso o resultados deficientes, pero en otras ocasiones tendremos que pensar profundamente para anticipar el daño que las tecnologías emergentes podrían causar”, afirma la investigadora. Sostiene además que el hecho de que a menudo haya desacuerdo en cuestiones de ética no significa que no haya una conclusión ética ‘correcta’ a la que podamos acceder con el enfoque correcto.

#### Es importante valorar qué tareas requieren de la IA y cuáles de los humanos

######## 2.2.2

###### Educación

Al igual que los ponentes en el foro, la CE habla en sus directrices de la necesidad de debate público y de implicar a las partes interesadas en todo el ciclo de vida de los sistemas de IA, y añade algo más a la ecuación: la educación. “Para garantizar la fiabilidad de la IA es necesario, más allá de desarrollar un conjunto de normas, crear y mantener una cultura y una mentalidad éticas a través del debate publico, la educación y el aprendizaje práctico”, dice el documento.

La CE habla de promover la formación y la educación, de manera que todas las partes interesadas sean conocedoras de la IA fiable y reciban formación en la materia. El emprendedor **Andrés Torrubia** está de acuerdo. “La IA es una tecnología que va a impactar a prácticamente todas las industrias y sectores sociales. Si el software ha transformado multitud de industrias en los últimos 20 años, la IA va a ser un tsunami con un poder aún mayor”, afirma.

Dice animar a la gente a aprender IA por dos motivos.  
El primero: por responsabilidad social. “La industria tecnológica está dominada por empresas que hasta ahora apenas han tenido que preocuparse por los impactos indirectos de sus acciones. En otras industrias, si por ejemplo una empresa fabrica un producto y vierte contaminación a un río, todos vemos que no es aceptable y como sociedad hemos impuesto mecanismos para minimizar lo que se llaman costes externos (costes de un producto o servicio que acaba asumiendo la sociedad y no quien produce el producto o servicio)”, explica.

---

#### Si el software ha transformado multitud de industrias en los últimos 20 años, la IA va a ser un tsunami con un poder aún mayor

La industria del *software*, y aún más la IA, es inmadura en lo relativo a los costes externos o externalidades, argumenta el emprendedor. “Empezamos a verlo con frecuencia en relación con el abuso de muchas tecnologías (adicciones, depresiones, pérdida de capacidad de atención, desinformación, manipulación a escala, etc.), que la IA sirve para amplificar, ya sea para lo malo como para lo bueno”, afirma.

Torrubia se considera un optimista tecnológico, pero un pesimista político. “Veo mucha gente hablando de ética, sesgos, gobernanza, responsabilidad respecto a la IA y, salvo excepciones, pocos saben realmente de lo que están hablando”, opina. Por eso le gustaría que hubiera más diversidad en los debates sobre la IA, para lo cual es necesario que otras profesiones (abogados, filósofos, economistas, etc.) se formen en profundidad. “Sé que es difícil, pero la alternativa -que hablen sin saber e incluso acaben redactando leyes- es inaceptable”, asevera.

El segundo motivo por el que se empeña en motivar a otros a aprender sobre IA tiene que ver con la modernización del tejido industrial español. “Nuestro IBEX 35 se parece más al IBEX de los años 90 en composición (constructoras, energía, bancos, etc.) que al del motor transformador del siglo XXI: empresas de desarrollo de tecnología basadas en software y, ahora, en IA. Los empleos que generan estas empresas son de calidad y falta talento”, comenta. De ahí la necesidad de formar más talento técnico.

El propio Torrubia, ingeniero de formación, estudió IA y aprendizaje automático a través de una plataforma de enseñanza *online*. “Este tipo de plataforma tiene tres factores a su favor: uno es la velocidad con la que adaptan contenidos a lo que hay en el mercado, otro es la disponibilidad de profesores y materiales excelentes más allá de lo que pueda estar accesible o no en tu lugar de residencia y el tercero es la flexibilidad de aprender a tu ritmo”, asegura.

Los ejecutivos también deben formarse en estas tecnologías. O al menos eso cree **Michael Li,** fundador y presidente de **[The Data Incubator](https://www.thedataincubator.com/).** Con esa misión creó esta empresa dedicada a la capacitación y la colocación en el ámbito de la ciencia de datos. “Ayudamos a las empresas a desarrollar una economía basada en datos mediante  
la contratación de científicos de datos y mediante la formación de sus equipos técnicos y también de los equipos gerentes y de otros profesionales en lo relacionado con el impacto de *big data* y la Inteligencia Artificial en sus negocios”, afirma.

Ante el hecho de que compañías como Google, Amazon y Netflix, que saben cómo capturar el valor de los datos, dominan el panorama competitivo, ser alfabetizado en datos es un imperativo competitivo, comenta Li. Y lo es tanto para las empresas como para sus empleados. “Antes de que las empresas puedan monetizar los datos, primero deben entenderlos. Muchos de nuestros clientes acuden a nosotros porque han descubierto que la gerencia media está aplastando las iniciativas basadas en datos porque no las comprenden, priorizan o valoran”, afirma.

---

#### Antes de que las empresas puedan monetizar los datos, primero deben entenderlos

Li señala que en The Data Incubator han visto un aumento en la demanda de su curso ‘El negocio de la ciencia de datos’ (Business of Data Science) a medida que más empresas y profesionales se dan cuenta de que lo necesitan. En el curso forman, por ejemplo, sobre cuál puede ser el impacto de la IA en un negocio, priorización de proyectos, gestión desde el principio hasta el final… “Cosas importantes que los ejecutivos y los gestores tienen que entender si quieren liderar una compañía basada en los datos y en IA”, afirma.

En cuanto a la parte técnica, Li comenta que el propósito es formar a expertos en lo que denomina “analítica de datos 1.0” -que en la anterior ola tecnológica trabajaban con cantidades de datos relativamente pequeñas, muy bien formuladas- para que sean capaces de trabajar en “analítica de datos 2.0” con las nuevas herramientas y habilidades necesarias para funcionar en el mundo de la Inteligencia Artificial.

Li cree que pensar en la automatización y el desempleo como si fuera una guerra entre humanos e Inteligencia Artificial, de personas contra máquinas, es un error. “El futuro va a ser una combinación de los dos; hay que dejar que ambos se dediquen a aquello en lo que tengan una ventaja comparativa, económicamente hablando”, afirma. Probablemente -dice- los ordenadores y la IA serán mejores para cosas como la constancia, la estadística. Los humanos estarán ahí para proporcionar contexto y empatía, añade.

La capacitación en ciencia de datos puede ser también diferencial en regiones emergentes. En el caso de África, la tecnóloga y emprendedora **Juliana Rotich,** asociada al MIT Media Lab, destaca la importancia de invertir en las capacidades locales de la IA, con planes de estudio de formación en ciencia de datos presentes tanto en escuelas como en universidades. “Los planes de estudio en la mayoría de las universidades africanas deben satisfacer las necesidades de los países. África está en proceso de ponerse al día”, afirma.

Subraya que, si bien esta puede ayudar a canalizar un flujo de soluciones para los problemas africanos, no todos los problemas se beneficiarían de ella, por lo que también

será necesario un cierto discernimiento entre qué puede aportar valor y qué puede causar daño. De acuerdo a esto- dice- las motivaciones y objetivos del desarrollo de estas tecnologías deben responder a necesidades sociales y no solo al imperativo de los accionistas. “Los ecosistemas de innovación deben fortalecerse con una inversión continua en nuevas empresas y reforzando también la capacidad de los gobiernos africanos para comprender de manera efectiva los beneficios y los riesgos del uso de la IA”, concluye.

#### Las motivaciones y objetivos del desarrollo de la IA deben responder a necesidades sociales

######## 2.2.3

###### Derechos digitales

Los riesgos de los que habla Rotich están relacionados, entre otras cosas, con la violación de los derechos de las personas. En el marco de la conversación sobre el impacto ético de la IA y de la tecnología en general, han emergido numerosas iniciativas que reclaman la consideración de  
los derechos digitales como una extensión de los derechos humanos. Ese es el objetivo de la conferencia **[RightsCon](https://www.rightscon.org/)** que organiza cada año -desde 2011- la organización **[AccessNow](https://www.accessnow.org/).** También lo es el de la **[Declaración de la Coalición de Ciudades por los Derechos Digitales](https://citiesfordigitalrights.org/sites/default/files/DeclarationCitiesDigitalRights_es-ES_0.pdf),** que en su tercer punto -dedicado a la “transparencia, responsabilidad y no discriminación de datos”- hace alusión al derecho de cualquier persona a acceder a información comprensible y precisa sobre los sistemas de inteligencia tecnológica, algorítmica y artificial que tienen impacto en su vida, y a la capacidad de cuestionar y cambiar sistemas injustos, sesgados o discriminatorios.

Más allá va el **[Ranking de Derechos Digitales](https://rankingdigitalrights.org/)** (RDR) que, entre otras cosas, realiza un Índice de Responsabilidad

---

Corporativa para poner de manifiesto compromisos, políticas y prácticas empresariales que afectan la libertad de expresión y la privacidad de los usuarios. El ***[2019 RDR Corporate Accountability Index](https://rankingdigitalrights.org/index2019/assets/static/download/RDRindex2019report.pdf)*** ha evaluado a 24 compañías en base a 35 indicadores que examinan diferentes aspectos de su gobierno, políticas y prácticas. Su medidor relativo al potencial impacto adverso de sus operaciones comerciales sobre los derechos humanos se ha ampliado. Este año pretenden abordar los esfuerzos de diligencia debida de las compañías con respecto al uso de herramientas automatizadas para la toma de decisiones (basadas en IA) y a sus políticas y prácticas publicitarias específicas.

Según el informe, solo tres empresas, -Deutsche Telekom, Microsoft y Telefónica- revelaron estar evaluando los riesgos de derechos humanos asociados al uso de IA. Telefónica fue la única que mostró que dicha evaluación es parte de su proceso formal y continuo de estimación de impacto en los derechos humanos. Esta compañía está por delante del resto en el Índice RDR con respecto al resto de compañías de telecomunicaciones, dado que -dice el documento- reveló significativamente más que sus pares sobre políticas que afectan la libertad de expresión y la privacidad. También aplicó mejoras “por un amplio margen”, superando al resto de empresas del Índice.

En base a los resultados obtenidos, el Índice RDR recomienda a las empresas realizar evaluaciones de impacto sobre los derechos humanos; fortalecer la supervisión directa sobre los riesgos relacionados con la seguridad, la privacidad y la libertad de expresión del usuario; comprometerse con la evaluación de terceros basada en estándares internacionales de derechos humanos; establecer mecanismos efectivos y accesibles de reclamación y solución e implicarse con las partes interesadas afectadas para crear nuevos procesos para identificar riesgos y mitigar daños.

¿Cuál es el papel de los gobiernos en todo esto? Para Jade Leung, el reto de la gobernanza de la IA “es un problema con solución que se puede lograr con muchísimo esfuerzo”. Primero, cree que deberíamos fijarnos en la historia, dado que no es la primera vez que una tecnología se ha politizado ni se ha vuelto estratégica. “La tecnología nuclear, la biotecnología, la criptografía… en todos los casos hemos tenido aciertos y errores, y deberíamos aprender de ellos”, asegura. La investigadora considera clave tener en cuenta dichas analogías.

En segundo lugar, Leung sostiene que se debería reforzar el impacto de las instituciones en las que más confían las personas. Como parte su trabajo en relación con GovAI, han llevado a cabo una encuesta al público estadounidense sobre varias cuestiones relacionadas con la IA. De ella se desprende que la gente confía muy poco en Facebook, pero mucho en las Fuerzas Armadas o en los investigadores. “Estos son los datos que deberíamos estudiar para ayudar a los centros de investigación a influir más en la gobernanza, y esto también fomenta la confianza en estas instituciones, lo cual considero esencial”, afirma.

La experta devuelve la pelota al sector privado: “Tiene que desempeñar un papel fundamental en materia de gobernanza. Me frustra ver cómo se considera hoy en día a las grandes empresas tecnológicas, que son prácticamente las malas de la película. Creo que tenemos que entender que la legislación gubernamental nunca ha ido ni irá al ritmo de los problemas de gobernanza tecnológica. Estas tecnologías se desarrollan en el sector privado, ahí es donde encontraremos a los expertos técnicos que nos ayudarán a hacerlas más seguras, resistentes y transparentes, siempre y cuando tengan los incentivos adecuados”, afirma.

En su opinión, un posible enfoque sobre la gobernanza tecnológica es poner en primer plano a las empresas del sector privado y dejar que lideren, pero pedirles que lo hagan de manera responsable para todos. Markou, como ya señalaba anteriormente, no cree que sea posible. El investigador compara el caso con la crisis financiera de 2008. “Se permitió que una industria carente de regulación

---

y sin control sobre sus excesos llevara a la quiebra de la economía global. Eso no puede pasar con la IA”, afirma.

En el marco de los derechos humanos, el Índice RDR considera que los gobiernos no solo deben protegerlos, sino que tienen un papel relevante que desempeñar para garantizar que las empresas ejerzan una gestión y supervisión adecuadas de los riesgos que puedan causar. Sus recomendaciones: una ley de protección de datos sólida para proteger la privacidad [como el RGPD en el caso europeo], implementar y exigir transparencia, o asegurar el acceso adecuado con mecanismos accesibles y efectivos de reclamación y recursos, entre otras.

######## 2.2.4

###### Propiedad de datos

Otra cuestión a debate es si las empresas deben pagar a los usuarios por sus datos, dado que estos son la base del modelo de negocio de muchas de las grandes tecnológicas. El columnista económico de The New York Times, **[Eduardo Porter](https://www.fundacionbankinter.org/ftf/expertos/eduardo-porter),** cree que vale la pena explorar este modelo basado en la propiedad de los datos. Dice no estar seguro de cuán valiosos son los datos marginales para los modelos de negocio actuales, impulsados por la publicidad. Sin embargo -añade- es posible que sí sean mucho más valiosos en una economía fuertemente basada en la IA.

Independientemente de si las empresas pagan o no por ellos, Porter reafirma lo comentado por otros ponentes del *think tank*: las personas necesitan entender mejor cómo se utilizan sus datos y tener más control sobre el proceso. Por otra parte, el reportero comenta que, si la IA impulsa el crecimiento de la productividad, aumentará la riqueza, y ello conlleva desafíos sociales y políticos. Entre ellos cita el de cómo garantizar que esta tecnología se despliegue para mejorar la productividad (en lugar de aprovecharse de ventajas fiscales, por ejemplo) y que la nueva riqueza no termine en el bolsillo de un puñado de plutócratas, sino que llegue a la gran mayoría de la población.

#### Las personas necesitan entender mejor cómo se utilizan sus datos y tener más control sobre el proceso

**Nuria Oliver** sostiene que hay un coste social que las empresas no están pagando, y pagarlo podría ser una forma de equilibrar la balanza. La cuestión es cómo, teniendo en cuenta que a algunas de estas compañías se las acusa precisamente de eludir impuestos. El parlamento francés ha aprobado un gravamen del 3% sobre los ingresos que empresas como Google y Facebook obtienen dentro del país, alegando que estas actualmente explotan lagunas fiscales globales para reducir su contribución a Hacienda.

También se proponen otras opciones como dotar de personalidad jurídica a las máquinas que reemplazan trabajo humano. Markou cree que esta es “una idea tonta, peligrosa y degradante”. Esto no significa que no se pueda crear algún tipo de mecanismo de rendición de cuentas, pero no necesitamos utilizar la personalidad como el punto de partida para construirla. “Es verter vino nuevo en botellas viejas”, concluye.

######## 2.2.5

###### Inteligencia Artificial para el bien social

Adam West, que nos hablaba en el anterior apartado sobre cómo las empresas pueden usar los datos para vender más, ejemplifica cómo esos mismos datos se podrían emplear para predecir qué probabilidad tiene una persona de sufrir una afección cardiaca. Es solo un ejemplo de cómo pasar de un pensamiento de beneficio meramente económico a uno social puede cambiar por completo el impacto de la IA.

En esta segunda categoría se enmarca la *AI for good*, o IA para el bien social. “La IA tiene el potencial de ayudar a abordar algunos de los mayores desafíos sociales del mundo”, comienza el informe ***[Applying artificial intelligence for social good](https://www.mckinsey.com/~/media/mckinsey/featured%20insights/artificial%20intelligence/applying%20artificial%20intelligence%20for%20social%20good/mgi-applying-ai-for-social-good-discussion-paper-dec-2018.ashx)*** de McKinsey. En él, la consultora recopila una biblioteca de aproximadamente 160 casos de uso, que sugieren que las capacidades existentes podrían contribuir a abordar los 17 objetivos de desarrollo sostenible de las Naciones Unidas (los ODS) “de lo que podrían beneficiarse cientos de millones de personas, tanto en países avanzados como emergentes”.

Los casos analizados van desde diagnosticar el cáncer hasta ayudar a las personas ciegas a navegar por sus alrededores, identificar a víctimas de explotación sexual online o ayudar en tareas de socorro (como las inundaciones tras el huracán Harvey en 2017). Sin embargo, y como ya era de prever, la IA no es una varita mágica. “Es solo una parte de un conjunto de herramientas de medidas mucho más amplio. Por ahora, cuestiones como la accesibilidad a los datos y la escasez de talento formado en IA limitan su aplicación para el bien social”, señala el documento.

---

Desde una perspectiva de desarrollo tecnológico y con el propósito de contribuir a los ODS a través de la IA y la ciencia de datos, trabaja **Kush Varshney,** codirector del programa de IBM ‘Ciencia para el bien social’ **([IBM Science for Social Good](https://www.research.ibm.com/science-for-social-good/)).** “Creemos que con la proliferación de tecnologías que están transformando nuestro mundo, como la IA, los esfuerzos para mejorar el bienestar social y abordar los desafíos globales pueden integrarse en el tejido de la investigacióny el desarrollo tecnológico y abordarse de manera sostenible y escalable”, afirma.

Para asegurarnos de que la tecnología para el bien realmente tenga un impacto medible -dice- debemos ir más allá de la práctica actual de crear soluciones a medida y pasar a un paradigma de plataforma abierta que proporcione capacidades fundamentales que muchas organizaciones de cambio social con misiones y necesidades similares puedan utilizar. Cree que ello requiere de un nuevo modelo de filantropía que reúna a tecnólogos, fundaciones y grupos de organizaciones por el cambio social.

---

######## 03

## Potencial y escenarios futuros de la Inteligencia Artificial

#### Hemos hablado de las aplicaciones de la IA, de las amenazas, riesgos y retos que conlleva y de posibles vías de solución. Ahora miramos al futuro para preguntarnos qué nos depara la IA. En este capítulo observamos ese porvenir desde dos perspectivas: primero desde la técnica; después desde la aplicada a la vida y a los diferentes escenarios que podrían darse, atendiendo a los indicios anteriormente expuestos y a las predicciones de los expertos en el Future Trends Forum.

######## 3.1

##### Potencial técnico

¿Hasta dónde puede llegar la IA a nivel técnico?  
Las respuestas se dividen en dos bandos. Por un lado, el de los superoptimistas tecnológicos que hablan de la superinteligencia artificial que podría volverse más poderosa que cualquier cosa que este planeta haya visto y que será  
un desafío existencial para la humanidad como especie. Siguen la corriente de pensamiento del filósofo **[Nick Bostrom](https://nickbostrom.com/),** director fundador del Instituto para el Futuro de la Humanidad de la Universidad de Oxford **([FHI](https://www.fhi.ox.ac.uk/))** y autor de *Superinteligencia: Caminos, peligros, estrategias* (TEELL, 2016). Una corriente alimentada por comentarios de los mediáticos Stephen Hawking o Elon Musk, entre otros.

#### Los optimistas tecnológicos creen que la IA podría volverse la tecnología más poderosa del planeta

La otra corriente es la de los escépticos, en la que se enmarca otro filósofo de la Universidad de Oxford, el profesor **Luciano Floridi,** director del **[Laboratorio para la Ética Digital](https://digitalethicslab.oii.ox.ac.uk/).** “Después de tanto hablar sobre los riesgos que plantean las máquinas ultrainteligentes es hora de poner luz sobre el asunto, dejar de preocuparnos acerca

---

de escenarios de ciencia ficción”, señalaba en un ensayo publicado en Aeon en 2016.

Entre esos escenarios, está el temor a que la IA pueda avanzar y automejorarse de tal manera que se vuelva difícil de controlar. Algo que descarta **[Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio):** “No es así como se construye la IA. El aprendizaje automático requiere de un proceso lento y minucioso para adquirir información a través de millones de ejemplos. Una máquina se mejora a sí misma, sí, pero muy, muy lentamente, y de formas muy especializadas. Y el tipo de algoritmos usados no se parece en nada a pequeños virus que se autoprograman”, aclara.

Floridi afirma que “la realidad de la IA es más trivial de lo que pensamos”. Por eso critica los movimientos que profetizan sobre la singularidad tecnológica: que los humanos algún día seremos superados por máquinas artificialmente inteligentes o por una inteligencia biológica mejorada cognitivamente, o por ambas. A juicio del filósofo e ingeniero, la que denomina ‘Iglesia de los Singularitarios’ está “distrayendo irresponsablemente” con la difusión de sus ideas.

Floridi también critica al bando opuesto -la ‘Iglesia de los Ateos’- por enredarse con los ‘singularitarios’ en una discusión “sin sentido”. Como el propio Turing escribió en 1950 en su conocido artículo ‘¿Puede una máquina pensar?’, la propia pregunta es demasiado inútil para merecer una discusión. Y, además -dice el filósofo- ambos se equivocan. “Una IA que piense de verdad, general y fuerte, no es lógicamente imposible, pero sí totalmente inverosímil. No tenemos idea de cómo diseñarla, sobre todo porque tenemos muy poca comprensión de cómo funciona nuestra propia inteligencia y nuestro cerebro”, asegura.

Esto significa -en su opinión- que no deberíamos perder el sueño en la posible apariencia de una ultrainteligencia. “Cualquier visión apocalíptica de la IA puede ser ignorada”, afirma. “Lo que realmente importa es que la creciente presencia de tecnologías más inteligentes está teniendo enormes efectos en cómo nos concebimos a nosotros mismos, el mundo y nuestras interacciones. Lo importante no es que las máquinas sean conscientes, inteligentes o capaces de conocer algo como lo hacemos nosotros. No lo son. Ninguna entidad consciente emergerá de una máquina de Turing”, sostiene el experto.

Lo relevante desde su punto de vista es que cada vez hay más tecnologías que son capaces de lidiar con más tareas mejor de lo que lo hacemos los humanos, y que eso nos desplaza de nuestra posición de antropocentrismo. Es lo que denomina “la Cuarta Revolución de nuestro propio entendimiento”. Ni somos el centro del universo (Copérnico), ni del reino biológico (Darwin), ni de la racionalidad (Freud). Y ahora tampoco lo somos de la infoesfera (Turing), sino que la compartimos con las tecnologías digitales, que define como “artefactos ordinarios que nos superan en cada vez más tareas, a pesar de no ser más inteligentes que una tostadora”. “Sus habilidades son humildes y nos hacen reevaluar la excepcionalidad humana y nuestro papel especial en el universo, que sigue siendo único”, añade.

#### Cada vez hay más tecnologías que rinden mejor los que humanos

Nuestro experto **David Weinberger** coincide con esta postura y tampoco cree que la superinteligencia sea el objetivo evolutivo de la IA: “No creo que los sistemas de IA existentes sean inteligentes ni conscientes, ni que se dirijan hacia cualquiera de esos dos estados. Las computadoras son procesadores de símbolos. Los cerebros, como los estómagos, no lo son”, asegura.

Ramón López de Mántaras también advierte -como señalábamos en el capítulo 1- de los riesgos de guiarse por “las sobre promesas de la IA”. No obstante, cree que tal vez en unos años estos sistemas sí sean capaces de adquirir cierta comprensión, aunque el primer reto, por ahora, es el de desarrollar sistemas multitarea, capaces de aprender múltiples tareas y evitar el desafío persistente que supone el denominado ‘olvido catastrófico’.

El concepto de olvido catastrófico -que introdujimos brevemente en el capítulo 1- se refiere a la dificultad de

---

enseñar nuevas habilidades al sistema para realizar nuevas tareas sin perder las funciones aprendidas previamente. Por ejemplo, si una red entrenada inicialmente para distinguir entre fotos de brazos y piernas se vuelve a entrenar para distinguir entre brazos y manos, sobre escribirá la información anterior y perderá la capacidad asociada a ella. Mántaras cree que en 10 años tendremos versiones de la IA más amplias que la actual, capaces de superar este obstáculo.

#### El olvido catastrófico es la dificultad de enseñar nuevas habilidades al sistema sin que se pierdan las anteriores

Otra barrera para la IA que los investigadores y tecnólogos trabajan por sortear es la conocida como ‘paradoja de Moravec’: las máquinas son capaces de hacer cosas que pensamos que son difíciles, como jugar al ajedrez, pero incapaces de aprender, como los humanos, habilidades psicomotrices o perceptivas que hasta un bebé tiene. De ahí que investigadores y tecnólogos estén tratando de entender cómo los niños pueden hacer fácilmente lo que hacen, con iniciativas como **[Quest for Intelligence](https://quest.mit.edu/)** del MIT.

#### La “paradoja de Moravec” se refiere a que la IA tiene habilidades intelectuales, pero no psicomotrices o perceptivas

El director inaugural de este ambicioso proyecto, el español **[Antonio Torralba](https://web.mit.edu/torralba/www/),** explica que una de sus líneas de estudio busca comprender el desarrollo de un niño hasta que tiene un año y medio. “Desde el punto de vista de las interacciones con el mundo, con tan solo un año un bebé ya es experto en muchas cosas: noción tridimensional,

precisión mecánica para controlar objetos, agilidad... Eso es algo que un robot no es capaz de hacer, o que le costaría muchísimo”, explica. Por eso, su reto es entender las sutilezas que explican el proceso de desarrollo infantil, “el mismo que hace que los bebés japoneses puedan entender la diferencia entre la letra ‘r’ y la ‘l’, pero los japoneses adultos no”, afirma Torralba.

Si bien el propio Torralba define esta línea de trabajo como uno de sus *moonshots* -que ambicionan a conseguir algo que a priori parece imposible, al menos en el corto plazo- hay otros campos en los que los científicos creen que la IA mejorará de forma más notoria en los próximos años. Mántaras cree que el aprendizaje automático será capaz de combinar mejor más y más técnicas, que seremos menos dependientes de los datos y que la IA será también más explicable, segura en términos de privacidad, sostenible y con mejor noción y conciencia de contexto cultural. Para llegar hasta ahí, dice, mantener a las personas en el proceso no solo será lo correcto, sino un aspecto esencial. “La colaboración entre humanos y máquinas será clave”, asegura.

Un factor esencial en esta colaboración, tanto para la comprensión de los algoritmos como para la confianza en ellos, es la explicabilidad, de la que ya hemos hablado en el anterior capítulo de la mano de Oliver Smith, director de Estrategia en Telefónica Alpha. Smith comenta que cuando Alpha Health trabajó con el Sistema Nacional de Salud (NHS) de Reino Unido, este insistió a la empresa española en que no podían usar modelos de aprendizaje profundo basados en redes neuronales. Tuvieron entonces que adaptar su tecnología a algo más simple, usando árboles de decisiones, que el personal del NHS podría entender.

---

#### La explicabilidad es un desafío para la IA, ya que implica que cada resultado deba poder explicarse

“Funcionó en ese momento, pero no funcionará siempre”, subraya Smith. Por eso, su equipo está estudiando cómo convertir redes neuronales en árboles de decisiones para no tener que renunciar a una tecnología más avanzada. “Hemos recurrido a una técnica de la década de los años 90, llamada TREPAN, muy prometedora”, comenta. El problema es que solo funciona con redes neuronales que van en una dirección, y no con aquellas que permiten conexiones arbitrarias o en bucle (redes neuronales recurrentes). Por otro camino discurren los esfuerzos de IBM en este sentido, que han protagonizado la más reciente [agosto de 2019] aportación al campo, con un set de recursos de programación de código abierto llamada ‘AI 360 Explainability’. Consiste en un conjunto de ocho diferentes algoritmos con algunos tutoriales-guía para ayudar a entender en qué tipo de casos conviene aplicar uno u otro o a qué audiencia se dirige la explicación.

Como apunta en un **[artículo en ZDnet](https://www.zdnet.com/article/ibm-offers-explainable-ai-toolkit-but-its-open-to-interpretation/)** Tiernan Ray, esta área requerirá aún de mayor investigación y exploración. “El set de herramientas de IBM es tan solo el comienzo de un duro trabajo”, afirma. Smith cree que este problema podría resolverse de aquí a 2030 y que ello dará un gran empujón a la Inteligencia Artificial y a su aplicación en materia de salud. “Pero no servirá para nada si no recordamos el papel fundamental del humano en ello”, sentencia.

Otros expertos del foro expusieron en profundidad cómo se espera que avancen otros aspectos de la tecnología en relación con la IA, que veremos a continuación:

######## 3.1.1

###### Ciencia de datos

¿Qué es la ciencia de datos? Lo explica **Nuria Oliver** que, además de ser miembro de la RAI, es parte de la **[Data-Pop Alliance](https://datapopalliance.org/),** entre otras muchas afiliaciones, y ha trabajado durante 25 años en modelos computacionales de comportamientos humanos individuales y agregados. “La ciencia de datos es una disciplina que ha surgido en  
los últimos diez a quince años en la intersección entre matemáticas y estadística, informática, administración de empresas y conocimiento del dominio”, describe. Asegura

además que es una profesión muy demandada. Tanto que el pasado agosto de 2018 hubo más de 151.000 vacantes para esta especialidad no cubiertas en EE.UU, según un **[informe de LinkedIn](https://economicgraph.linkedin.com/resources/linkedin-workforce-report-august-2018).**

Dentro de la ciencia de datos -explica Oliver- hay un campo nuevo que es la ciencia social computacional, que se apalanca en datos conductuales a gran escala, agregados, para corroborar o no las teorías de las ciencias sociales. Y dentro de las ciencias sociales computacionales, hay un área en la que esta ingeniera ha estado trabajando los últimos diez años: la ciencia de datos para el bien común. El objetivo es apalancarse en el análisis y procesamiento de ingentes conjuntos de  
datos para tomar mejores decisiones en áreas como la salud pública, la inclusión financiera, el cambio climático o la ayuda en emergencias ante, por ejemplo, desastres naturales, lo cual “podría salvar millones de vidas”, afirma.

Como su propio nombre indica, la ciencia de datos necesita datos y ciencia. Desde el punto de vista de los datos,  
Oliver asegura que los informes dicen que cada año actual generamos más datos que en los 5.000 años anteriores.  
“La capacidad de interpretarlos es, además, un activo hipervalioso en la economía de hoy”, afirma. De acuerdo con un **informe** de la Comisión Europea, el valor de la economía europea de datos superará los 700 mil millones en la Unión Europea el año que viene, “siempre que las políticas y las condiciones legislativas favorables se implementen a tiempo y se fomenten las inversiones en TIC”.

La ciencia de datos también necesita ciencia. Es bueno tener los datos, pero, si no sabemos qué hacer con ellos, son basura digital. Hay que saber interpretarlos, y estamos hablando de cantidades ingentes de datos no estructurados, que son invisibles e incomprensibles sin la ciencia y ciertas disciplinas dentro de la misma, en concreto, el aprendizaje automático basado en datos.

“Ahí también tenemos suerte, porque en los últimos ocho años hemos visto una explosión de varios factores que han contribuido a un gran progreso en el aprendizaje

---

automático basado en datos, además de la disponibilidad de una ingente cantidad datos: ahora tenemos ordenadores de alto rendimiento asequibles y una arquitectura de aprendizaje automático muy sofisticada que permite ingerir los datos y entenderlos”, explica Oliver, en línea con lo que señalábamos ya en el capítulo 1.

En opinión de la ingeniera, estos factores son muy relevantes. “Estamos viviendo una democratización  
del aprendizaje automático basado en datos. Hoy día hay muchas herramientas y librerías que crean un nivel de abstracción tal que no hace falta programar todas las ecuaciones del algoritmo para poder entrenarlo. Básicamente son piezas de lego”, comenta. Pero Oliver advierte de hay que saber lo que estamos haciendo, “de lo contrario es algo peligroso, porque podríamos estar aprendiendo algo sin sentido”, afirma.

Los avances en estos ocho años han repercutido en logros a juicio de Oliver increíbles, como la a menudo citada victoria de AlphaGo de Google al campeón mundial de Go. Pero no solo hacen estas cosas, sino que afectan a nuestra vida diaria, como también hemos comentado ya. “El ámbito de impacto de los algoritmos basados en datos en nuestras vidas es inmenso, y va a seguir creciendo. Por tanto, diría que la ciencia de datos no es solo la suma de ciencia y datos. Necesitamos mucho más que eso. La ciencia de datos es multidisciplinar: necesitamos conocimiento sobre el dominio, equipos multidisciplinares y ética”, asegura.

La científica está convencida de que, si no se incluyen todos esos pilares en su desarrollo, la ciencia de datos no será,  
en general, buena para la humanidad. En este sentido, cita seis áreas de mejora en las que la comunidad científica en informática y ciencia de datos está trabajando:

1. Violación de la privacidad computacional
2. Exclusión o discriminación social sesgada
3. Asimetría en aptitudes informacionales (hay muchos datos, pero resulta que la mayoría de los datos están en manos privadas)
4. Opacidad y falta de transparencia
5. Veracidad
6. Ética

#### La ciencia de datos está trabajando en seis áreas de mejora

De acuerdo con esto, ¿cuál es el futuro de la ciencia de datos? En opinión de Oliver, pasa inexorablemente por incluir los algoritmos FATEN, con F de *fairness* (justicia),

que atienda a la no discriminación y a la cooperación; con A de *autonomy*, *accountability* y *augmented* (autonomía, rendición de cuentas y aumento), en referencia a la necesidad de preservar el valor humano de la soberanía,  
de una responsabilización clara y de los modelos que aumenten -y no reemplacen- a la inteligencia humana; con T de *trust* y de *transparency* (confianza y transparencia); con E de *education* y *beneficence* (educación y beneficencia); y con N de *non-maleficence*: minimizar los aspectos negativos, asegurar que hay un nivel mínimo de fiabilidad, seguridad, reproducibilidad, prudencia… siempre preservando la privacidad de las personas.

“No toda innovación equivale a progreso, y debemos apuntar al progreso, no a la innovación por el mero hecho de innovar”, argumenta Oliver. Tenemos que asegurarnos -dice- de que somos sostenibles, con diversidad y verdad; de que se educa a todos los niveles: niños, adolescentes, profesionales, políticos y, en general, personas que toman decisiones, que muchas veces deciden sobre temas que realmente no comprenden. “En resumen, el futuro de la ciencia de datos es tener fe en los algoritmos, centrarlos siempre en la persona e invertir en educación”, concluye.

######## 3.1.2

###### Comprensión del lenguaje natural

El campo de la lingüística computacional -el estudio de la lengua desde la perspectiva de la informática- tiene más de 70 años y aún un largo camino por recorrer. Desde las primeras traducciones realizadas por una máquina hasta la actual explosión de asistentes personales, chatbots, etc., la explosión de herramientas de procesamiento del lenguaje natural y de análisis del lenguaje en los años 2000 han propiciado una evidente evolución de este campo, con límites aún difusos.

Al estudio, desarrollo y aplicación de estas tecnologías se dedica **Pilar Manchón,** una filóloga española que encontró

---

su pasión en la lingüística computacional. Tras vender su pionera empresa de chatbots -Indisys- a Intel en 2013, y tras su paso como directora del área de Interfaces Cognitivas de Amazon, trabaja ahora como directora del área de IA en Roku, una plataforma de transmisión de contenidos de televisión y reproductor multimedia.

Manchón describe las herramientas en informática lingüística como “un paquete de tecnologías que se han unido bajo una misma categoría, pero que en realidad son muy diferentes y tienen requisitos distintos en cuanto a rendimiento, datos, etc.”. Entre ellas, destacan:

- El **reconocimiento de voz** transcribe el contenido de audio -la voz, lo que dice una persona- a una serie de palabras. Por otro lado, está la comprensión del lenguaje natural, que implica entendimiento del lenguaje, estableciendo correlaciones entre lo que vemos y lo que creemos. Manchón cree que en realidad estos sistemas ejercen una ‘competencia sin comprensión’, un postulado ampliamente compartido -y también discutido- en la comunidad científica, propuesto por el filósofo de la mente y científico cognitivo Daniel Dennett. Se refiere  
al hecho de que, a nivel funcional, un sistema puede alcanzar un nivel de rendimiento (competencia) que en contextos humanos se atribuiría a la comprensión (es decir, la inteligencia), pero sin comprenderlo.
- La **gestión del diálogo** es imprescindible para tener una conversación. “Hay que entender los turnos de palabra, saber a quién le toca hablar, cómo consigo relacionar lo que ha dicho la otra persona y la información que hemos compartido con lo que he dicho yo, y hacia dónde va  
la conversación según lo que ambas partes sepamos”, especifica la experta.
- **La generación del lenguaje natural** permite a la máquina comunicarse de forma que otro ser humano pueda comprenderla, teniendo una idea de qué va tratar dicha interacción y cómo transcurrirá. Generar una tarea,  
un mensaje, una información, sin importar el qué, en lenguaje natural. “Hablando claro, se traduce en construir una frase gramaticalmente decente”, comenta Manchón.
- **La síntesis del habla,** por su parte, hace que sea posible generar lenguaje natural en forma de voz.

¿Y qué es lo próximo? “La multimodalidad o la multitarea multimodal. Vamos a dejar atrás la voz y el texto, pensad en una visión más completa de un asistente virtual que pueda ver y sentir y sepa acerca de otras cosas aparte de lo que tú le dices. Si digo algo como ‘Enséñame eso’ o ‘Sube esa persiana de allí’, ese asistente virtual tendría que saber, literalmente, a qué estoy señalando para entender lo que le digo”, describe la lingüista. Así que, aparte del lenguaje, está la interacción de todas estas modalidades contextuales que necesitamos poner en común para saber cuál es la verdadera intención según el contexto en el que ocurran.

#### El lenguaje natural hace que la máquina pueda comunicarse con el ser humano

---

“Además de toda esta inteligencia funcional, tenemos un componente social y emocional porque los seres humanos somos criaturas muy necesitadas. Para depositar nuestra confianza en una entidad semi inteligente que desempeña un papel en nuestra vida, tenemos que tener algún tipo de conexión emocional. Si no, se la atribuiremos igualmente”, explica Manchón. Pone como ejemplo de ello el caso de María, una asistente virtual que Indisys desarrolló para el área de Salud de la Junta de Andalucía: “Los usuarios decían sentirse más cómodos ante la máquina a la hora de realizar consultas de información sexual porque no se sienten juzgados, pero a la vez hablaban de ella como si fuera una persona”, afirma.

#### Se desarrollarán asistentes virtuales que, además de oír, puedan ver y sentir

Otro componente -añade la experta- es la detección de personalidad: la capacidad para detectar el tipo de personalidad que uno tiene a través del lenguaje. Esto -dice- tiene que ver con el concepto de confianza: quién eres, de dónde vienes, qué es lo que sabes… La inteligencia inferida no sólo tiene que ver con lo que me dices. Teniendo en cuenta todas estas fuentes de información, se trata de entender cómo añadir una, dos y tres cosas más a lo que realmente estoy transmitiendo en un mensaje en particular y en cómo añadir toda esa información para dar un paso adelante de manera más inteligente. Y luego está la sintonización, que tiene más que ver con tener un comportamiento coherente para que las expectativas del usuario en cuanto a la interacción no se desajusten.

#### La IA tendrá la capacidad de detectar la personalidad a través del lenguaje

El futuro del campo pasa por juntar todas estas piezas para que encajen de forma correcta, e idealmente por hacerlo de forma justa y éticamente correcta, previendo y evitando efectos negativos, como señalábamos en el

capítulo anterior. Las necesidades técnicas que requieren el desarrollo de estas tecnologías a menudo chocan con aspectos delicados. “Los asistentes virtuales, sin ir más lejos, están presentes en todos los ámbitos de nuestra vida. Alexa, Google Home, Cortana, Siri, etc. están en casa, en el coche, en la oficina... podrían estar en cualquier parte, así que saben mucho de nosotros”, afirma Manchón.

La propiedad de datos, la gestión de datos, quién posee esos datos, cuántos datos estamos compartiendo, si estamos compartiendo más de lo que queremos, cómo dices lo que dices, cuándo lo dices, a quién se lo dices, en qué tono se lo dices… “Cuando pedimos una recomendación a Alexa, ¿nos dirá lo mejor para nosotros o para las empresas que han pagado por aparecer ahí? Cuando un asistente virtual nos ofrece recomendaciones, ¿en interés de quién son esas recomendaciones realmente?”, cuestiona. Dado que el aprendizaje automático aprende por observación ¿cómo explicamos al algoritmo si no nos gusta el patrón observado, que no lo queremos o que es incorrecto?, se pregunta la experta.

Manchón asegura que hay empresas que utilizan las conversaciones que tenemos, por ejemplo, con una empresa de telecomunicaciones, para determinar nuestro perfil: qué tipo de persona eres, si pueden ofrecerte sus productos con éxito, si pueden venderte algo o hacer venta cruzada contigo, si eres una persona que se enfada fácilmente… “Crean un perfil sobre ti y tú no sabes a dónde va a parar ese perfil. Podría afectar una solicitud de trabajo, una solicitud para una hipoteca, para la universidad, o tu seguro, o tus decisiones financieras. Por tanto, las empresas, sociedades y servicios que están fuera de tu alcance te juzgan como usuario basándose en información que no puedes controlar. Ni están cualificados para hacerlo, ni tienen permisos ni hay nada que valide esa información”, sostiene.

---

Por otra parte, estos sistemas pueden usarse con fines loables como detectar comportamientos de depredadores sexuales en la red, sobre todo con niños; o detectar acoso o cómo se manipula a la gente a través de las redes sociales. Las tecnologías lingüísticas también se pueden aplicar al análisis y clasificación de documentos y para ayudar en materia de educación, para hacerla accesible y más fácil para aquellos que no pueden estudiar. Manchón considera fundamental “extraer la complejidad de  
la Inteligencia Artificial a través del lenguaje para empoderar a otros y que puedan utilizarla, y así crecer más rápido y mejor”, concluye.

######## 3.1.3

###### Vehículos autónomos

Los vehículos autónomos han pasado de ser cosa de ciencia ficción a prototipos reales: coches por tierra y por aire, camiones, bicicletas, drones… Forman parte de las líneas de I+D de los grandes fabricantes automovilísticos y aeronáuticos, de recién llegados como Uber o Didi, de gigantes tecnológicos como Google y, como es obvio, de multitud de universidades, grupos y centros de investigación. Un nuevo mercado que, según las estimaciones de **[Allied Market Research](https://www.alliedmarketresearch.com/autonomous-vehicle-market),** crecerá de 54.000 millones de dólares en 2019 a 556.000 millones de dólares en 2026.

Si bien ya hay en marcha numerosas pruebas y servicios piloto con este tipo de vehículos, ya sea para el transporte de mercancías, paquetería o humanos, o incluso para salvamento, la gran pregunta es cuándo estarán estos vehículos y la infraestructura necesaria para su funcionamiento plenamente desarrollada y desplegada.

#### El coche autónomo ya ofrece servicios piloto para el transporte de mercancías, paquetería y humanos

**Raúl Rojas** - profesor e investigador asistente al foro que en el capítulo 1 nos introducía en el estado del arte de estos vehículos y en el capítulo 2 en

sus implicaciones éticas- cree que esto podría suceder en un plazo de 10 años, el plazo en el que, según su análisis, se suceden los cambios de paradigmas motivados por un salto tecnológico. “La transición de servidores a miniordenadores la motivó la introducción del circuito integrado, y el microprocesador la del paso de miniordenadores a ordenadores personales. Después llegaron los chips para las redes inalámbricas que nos permitieron implementarlas. Desde el 2010, se habla del IoT, y la cuestión es qué podemos esperar en los próximos diez años”, cuestiona.

En su opinión, ese próximo paradigma será la inteligencia bajo demanda. Habrá dispositivos inteligentes que tendrán Inteligencia Artificial, pero no en la nube, sino en un chip, tal y como ha explicado Lewis. Así que la inteligencia va a estar ahí, en los mismos dispositivos. Eso va a producir muchas aplicaciones nuevas, será “el internet de las cosas inteligentes”.

Rojas asegura que el proceso ya ha comenzado y considera un paso importante la integración de las telecomunicaciones en los coches, algo que -asegura- comenzará en 2020. “El próximo año la mayoría de las grandes empresas automotrices -BMW, Mercedes, Volkswagen, Volvo- van a introducir en el mercado lo que llaman ‘conducción automática de muy alto nivel’, que permitirá la autoconducción en autopista. Solo hará falta pulsar un botón y el coche se mantendrá en el carril”, comenta. Pero eso solo en autopista, ya que aún faltan herramientas para poder aplicarlo a la ciudad también.

Uno de los retos para la adopción de los coches autónomos en las ciudades es disponer de mapas más precisos sobre la configuración de las ciudades. Hay varias empresas que se dedican a ello, realizando mapeos 3D que ofrecen mucha más información que Google Maps- dice el investigador. “También necesitamos sensores más precisos y asequibles que los que tenemos hoy en día, telecomunicación entre coches a muy alta velocidad y una mejor predicción de los comportamientos”, añade.

Así que, ¿qué podemos esperar desde la perspectiva técnica? “La conducción automática de alto nivel está en la agenda de la mayoría de empresas, pero aún necesitamos millones de kilómetros de experiencia utilizando estos

---

sistemas, y creo que la experiencia en las autopistas va a servir como base para una autonomía completa en algunas zonas de las grandes ciudades, y esto tal vez suceda para 2030”, concluye Rojas su predicción.

#### Para desarrollar el coche autónomo se requieren mapas más precisos de las ciudades

En lo que respecta a la económica, señala que es necesaria una justificación. "Waymo dice que su justificación es que quieren ser una empresa de taxis, quieren ser como Uber, pero con ordenadores. Pero Uber y Lyft también quieren eso”, afirma. Por otra parte está la aceptación social, algo que Rojas cree muy importante. Por mucho que los vehículos autónomos (AV) se promocionen como alternativas más seguras a los dirigidos por humanos, estos últimos no los usarán si no confían en ellos. Según una encuesta de la organización AAA, las mujeres se sienten aún menos cómodas (79%) que los hombres (62%). Igualmente, un **[estudio del MIT](http://agelab.mit.edu/system/files/2018-12/2017_TRB_Abraham.pdf)** señala que el 53% de las mujeres (frente al 32% de los hombres) preferiría un asistente de conducción en lugar de un vehículo totalmente autónomo.

######## 3.1.4

###### Edge computing

Vivimos en la era de la computación en nube. Usamos nuestros ordenadores, tabletas y móviles no ya como sistemas de almacenamiento, sino para acceder a servicios de almacenamiento, trabajo y compartición de archivos *online* como Google Drive, Dropbox, One Drive o Slack. Individuos y empresas en todo el mundo confían en la infraestructura de unos pocos proveedores, esencialmente -aunque no solo- Google, IBM, Amazon y Microsoft. Frente a este sistema, se empieza a hablar de la informática perimetral o cercana al límite (más conocida por su voz inglesa: *edge computing*). Se denomina así a la computación que se realiza en la fuente de datos o cerca de ella, en lugar de confiar en la nube para hacer todo el trabajo. No significa que la nube desaparezca, sino que, de alguna manera, se acerca a nosotros.

¿Qué tiene que ver esto con el advenimiento de la IA y -más en específico- del aprendizaje automático? Lo explica **Anthony Lewis,** vicepresidente y jefe de Inteligencia

Artificial y del Laboratorio de Computación Emergente para tecnologías disruptivas en Hewlett-Packard. “El aprendizaje automático busca patrones en datos y su mayor problema es cómo dar respuesta a datos que no hemos visto antes”, comenta. Esto es: ser capaz de generalizar más allá de lo que se ha visto en un patrón de datos. Por ejemplo, que un algoritmo expuesto a fotografías de gatos sea capaz de identificar un gato al que no ha visto.

#### El aprendizaje automático busca patrones en datos y su mayor problema es cómo dar respuesta a datos que no hemos visto antes

Para ello, hace falta una representación o comprensión de algún tipo de los datos que nos llegan. Por ejemplo -dice Lewis- si tenemos una imagen de cinco megapixeles, podría haber una amplia gama de combinaciones sobre cómo mapear esos cinco millones de píxeles para que la respuesta sea gato o perro. Es una especie de representación interna, que se puede hacer a mano o aprendida. Y he aquí -afirma el experto- uno de los grandes hitos recientes en el aprendizaje automático: el uso de redes neuronales para aprender esa representación interna. “Esto conlleva implicaciones en todos los sectores económicos, y en cierta manera, nos ayuda a entendernos a nosotros mismos y a desmitificar qué significa el ser humano”, sostiene el directivo de HP.

---

¿Qué novedades hay? El progreso en aprendizaje automático lleva años de recorrido y se ha convertido en una parte central de la Inteligencia Artificial. Como ya hemos comentado en capítulos anteriores, la ingente cantidad de datos etiquetados disponibles ha contribuido considerablemente a ello. Pero ese aumento en datos -asegura Lewis- nos ha situado en una encrucijada: gran parte del aprendizaje automático hoy en día tiene lugar en la nube, pero el aumento del ancho de banda es mínimo.

“Los datos están atrapados en el límite, de tal forma que muchos datos tienen que procesarse al límite. Por ello, aumenta la necesidad de *hardware* específico para ello, aceleradores neuronales, que no son caros y en cambio tienen muchísima potencia”, explica Lewis. Esto podría -dice- llevar a una distribución de la capacidad en el entorno hacia el límite, donde tendríamos una potencia de procesamiento de *terabytes* (miles de GB) en el bolsillo, en el coche… Así que habrá una gran cantidad de procesamiento de datos que se da en torno a nosotros.

El experto habla de un efecto secundario resultante de la creación de estos procesadores, que han podido simular hasta 10.000 millones de neuronas. “La corteza cerebral humana tiene algo más de 20.000 millones de neuronas, por lo que la escala a la que estamos llegando se corresponde con la de los humanos. Al menos -puntualiza- en términos de números, ya que las simulaciones neuronales “no son ni de cerca tan realistas como las humanas”, afirma Lewis.

Otro aspecto que destaca es que estos avances se fundamentan “en un movimiento de fuente abierta sin igual”. “En cuanto publicas algo, se reproduce por otros grupos de investigación que pueden aportar a su mejora, lo cual lleva a una evolución rapidísima del campo”, asegura. La gente -comenta- también está aprendiendo a construir sistemas que se refinan solos, de tal forma que no hacen falta las aptitudes que hacían falta antes para dominar el aprendizaje automático. Considera que esto va a llevar a una democratización muy radical de la IA. “Las grandes empresas se van a quedar sin poder, y los chicos y chicas en un colegio mayor van a poder hacer cosas increíbles”, sostiene.

Otra cosa que estudian en HP es el ‘aprendizaje automático simbiótico’: cómo copiar y mejorar el rendimiento de las personas de forma simbiótica. Por ejemplo, monitorizan cómo interactúan con otras personas los agentes de un call center, y de ahí extraen modelos adaptables sobre cómo podrían hacerlo de forma más eficiente. Lewis cree que esta es una gran tendencia que va a despegar en los próximos años.

######## 3.1.5

###### Computación cuántica

**[Tanisha Bassan](http://tanishabassan.ca/),** una joven desarrolladora de programación cuántica nombrada en CES 2019 como ‘Young Innovator’s to Watch’, asistió al Future Trends Forum para hablar sobre computación cuántica y su relación con la IA, ya que uno de sus focos de atención -dice- es cómo el aprendizaje automático y la informática cuántica van a solaparse. Para entenderlo, es necesario tratar de entender en qué consiste esta última. “La computación cuántica es la idea de que se pueden aprovechar las propiedades de la mecánica cuántica -las leyes y normas que utilizamos para entender cómo funcionan las nanopartículas en el ambiente e interactúan unas con otras- para crear un ordenador que nos ayude a expandir nuestra potencia computacional”, explica Bassan.

---

Esta posibilidad - dice la estudiante e investigadora- ha hecho que se hable de una revolución de la informática clásica, limitada en sus posibilidades por el hardware. “Ahora vemos una cantidad relevante de tecnología en el ámbito de la informática cuántica que nos va a ayudar a expandir e ir más allá del significado mismo de la informática y de lo que puede conseguir”, asegura. Habla de una tecnología que puede simular la física de abajo a arriba, o ayudarnos a entender la complejidad del funcionamiento de la naturaleza, apalancándose en ideas de la propia naturaleza.

#### La informática cuántica puede simular la física y ayudarnos a entender la complejidad de la naturaleza

“La base de la informática cuántica consiste en apalancarse en la mecánica cuántica, que son las normas y leyes que usamos para entender cómo funcionan e interactúan entre sí las cosas a muy pequeña escala”, subraya. De esta destaca “dos propiedades muy útiles”. La primera es la superposición, que establece que se puede tener una partícula en cualquier estado exactamente al mismo tiempo. La segunda propiedad es el entrelazamiento: la idea de que puede haber dos partículas, dos puntos de información, que están correlacionados. Lo que afecta a una, afecta directamente a la otra.

“Estas dos propiedades -dice- son los cimientos de la mecánica cuántica, y la razón de que la computación a esta escala pueda llevar a una potencia computacional exponencial que permita resolver algunos de los problemas más difíciles del mundo”. Lo ejemplifica con una analogía: “Imaginen que hay una biblioteca repleta de millones y millones de libros, que son cantidades de conocimiento ingentes. Cojo un libro y escojo una página, que contiene información que quiero recuperar. La marco con una X. Si le pido a un ordenador tradicional que encuentre la X, ese ordenador clásico o superordenador podría tardar la longitud del universo, porque tendría que mirar cada una de las páginas de cada libro, una a una. Sin embargo, un ordenador cuántico podría entrar en la misma biblioteca y mirar todas y cada una de las páginas de todos y cada uno de los libros a la vez y encontrar la información, o la X, en mucho menos tiempo”.

Bassan cree que esto es particularmente interesante porque la mayor parte de toda la información creada y por crear son datos no estructurados, que no somos capaces de analizar ni de entender. Problemas complejos que son difíciles de simular matemáticamente con nuestros programas y nuestro hardware actual, pero que cree que la computación cuántica ayudará a desentramar. “Pongamos por caso el cambio climático, el hambre en el mundo, la crisis de los alimentos, la crisis inmobiliaria, el cáncer… Con los ordenadores cuánticos podríamos entrar en las ideas y conceptos que subyacen a ellos y mapear su complejidad, llevándola a un hardware que pueda entenderlos de formas que no éramos capaces de imaginar hasta la fecha”, sostiene.

Como ejemplos concretos de aplicación cita el descubrimiento de medicamentos. La programadora parte de la base de  
que actualmente carecemos de la capacidad computacional necesaria para simular con exactitud entornos reales -ya sea de un fármaco o de cualquier sistema físico- en un ordenador. “Si pudiéramos hacerlo y entender cómo interactúa el fármaco desarrollado en determinadas condiciones ambientales, se podría reducir drásticamente el tiempo que se tarda en investigar y desarrollar ese fármaco: de 10 o 15 años a un par de semanas. Esto permitiría abordar otros muchos problemas a los que nos enfrentamos hoy en día en cuanto al control de enfermedades”, afirma.

---

¿Qué tiene que ver con esto el aprendizaje automático? Sería en este caso -ejemplifica- la carrocería del coche  
que va a permitir acotar cómo abordamos la complejidad, usando diferentes técnicas aún por investigar y desarrollar y ofreciendo contexto para entenderla y tomar decisiones. La computación cuántica será -dice- el combustible que alimente el motor, que proporcionará la potencia necesaria para entender la complejidad a una enorme velocidad.

“Si el aprendizaje automático cuántico se convierte en la máquina del futuro, probablemente podrá solucionar -a largo plazo- algunos de los problemas más complejos del mundo”, insiste. Cree que será una revolución comparable a la de internet: “De igual modo que en los 50 no sabíamos a dónde nos llevarían los ordenadores ni hubiéramos podido concebir internet, no podemos saber qué harán en 50 años los ordenadores cuánticos, pero con seguridad será revolucionario”, sostiene. En un plazo más corto, en 10 o 15 años, cree que esta tecnología, cambiará cómo recopilamos los datos y cómo los entendemos, ayudará a resolver problemas de optimización para encontrar la mejor solución entre varias, y aumentará la potencia de cómputo en la mayoría de los sectores.

#### La computación cuántica ayudará a resolver problemas de optimización

######## 3.2

##### Escenarios futuros

Hemos pedido a nuestros expertos que se sitúen dentro de 10 años (2030) y piensen en cómo la IA y su desarrollo podrán afectar a nuestra vida diaria. En este ejercicio de prospección hablamos de los siguientes sectores:

######## 3.2.1

###### Empleo y sociedad

En el futuro mundo del trabajo, la Inteligencia Artificial cobra un papel fundamental, cambiando completamente la forma de trabajar y la productividad.

El Estado de bienestar ha cobrado mucha relevancia, ya que se ha implantado un tipo de Renta Básica Universal (RBU), gracias a la productividad de las máquinas y calibrada por un sistema de crédito social acumulable -que clasifica y

califica a los empleados- para medir la calidad de la vida, el acceso a medicamentos y otras variables. El trato es que quien acepta esta RBU con todos los beneficios debe renunciar a la privacidad y abrir sus datos.

Es este sistema el que gestiona la vivienda y proporciona por igual acceso a la información, a servicios bajo demanda (en un sistema donde el producto se ofrece como servicio), a ocio y videojuegos… y consigue que las personas estén satisfechas y generalmente felices.

Los retos a los que se enfrenta este sistema son:

- Cómo hacer que la RBU funcione y consiga el bienestar social.
- Cómo gestionar la vivienda como bien clave.
- Cómo incentivar la educación porque ya no la necesitaríamos para tener un trabajo.

######## 3.2.2

###### Educación

La Inteligencia Artificial se ha generalizado en el ámbito de la educación, lo que ha desembocado en un sistema de enseñanza más personalizada, que además integra la educación en Inteligencia Artificial en todas las etapas del currículo, sea lo que sea lo que se estudie, aunque a diferentes niveles y grados.

Los profesores aprenden técnicas de formación relacionadas con la IA, que permite reconocer a aquellos estudiantes que no van a ser adecuados para el mundo tecnológico, a quienes se forma en habilidades y profesiones asociadas al cuidado y lo emocional. Cada profesor enseña al individuo y no al grupo.

Los estudiantes disponen no solo de profesores, sino de tutores de IA personalizados. El foco es el desarrollo individual de cada cual, no aprender por el mínimo denominador común. Se prepara al estudiante para que sea un miembro productivo para la sociedad. Son trabajos complejos, creativos, emocionales… que permiten una mejor realización de su potencial.

El principal reto al que se enfrenta esta educación del futuro es que se deberían enseñar habilidades sociales porque la educación individualizada reduce al mínimo la socialización.

######## 3.2.3

###### Salud

Gracias a la IA y a los datos de los pacientes (incluidos dispositivos llevables – *wearables*-

---

conectados que enviarán información de salud de cada persona constantemente), los servicios sanitarios dispondrán de más tiempo para cada paciente, dedicarán menos horas a escribir informes y ofrecerán diagnósticos más precisos e individualizados.

Los médicos no serán reemplazados. En lugar de disponer de una IA que actúe como médico personal, tendremos asistentes sanitarios personales que ayudarán a la gente a cambiar su comportamiento todos los días (alimentados por los *wearables* que envían información a los médicos). En consecuencia, la gente cambiará sus hábitos de vida diarios de formas que ni siquiera nos podemos imaginar hoy en día.

Esto provocará un cambio de paradigma del sistema sanitario en general y de atención sanitaria en particular, que será no ya tratar enfermedades, sino prevenirlas. Predecir y prevenir en lugar de reaccionar a posteriori. Esto conducirá a una reducción de costes en el ámbito de la salud de urgencias y derivará en sistemas de salud más equitativos, además de mejores resultados de salud a largo plazo.

Los principales retos a los que se enfrenta este nuevo sistema sanitario son:

- Será necesario convivir constantemente con la tecnología y aprender a colaborar con ella, lo que requiere formación tanto para los médicos como para los pacientes.
- Se deberá diseñar la tecnología con la intención de mejorar y ayudar a los profesionales sanitarios, nunca reemplazarlos.
- Habrá, previsiblemente, mayores costes de salud asociada a atención de pacientes crónicos, cuidados, etc., especialmente teniendo en cuenta el aumento de la esperanza de vida y el consiguiente envejecimiento de la población.

######## 3.2.4

###### Administración Pública

La tecnología pondrá a disposición de la sociedad y de la IA multitud de datos (ciudades, carreteras, hospitales, escuelas, etc.) que facilitarán la toma de decisiones. Esto provocará un progreso en la eficiencia de los servicios públicos, mejorando el acceso a la vivienda, la seguridad alimentaria y los servicios de salud; así como los derechos digitales y la protección de datos; que reducirán la corrupción y

establecerán derechos de propiedad más precisos.

Las personas tendrán más visibilidad por lo que habrá un empoderamiento de las comunidades vecinales, que podrán decidir directamente sobre dónde y cómo quieren vivir. Esto hará que las ciudades sean más habitables y vivibles.

Los retos que presenta son:

- La disponibilidad de información pública generará mayor demanda social hacia el gobierno sobre cualquier problema, sin considerar la priorización de las necesidades.
- Habrá que observar el desarrollo de la toma de decisiones ciudadanas, sobre todo en lo referente a la autoorganización y autogestión de comunidades y vecindarios.

######## 3.2.5

###### Gestión empresarial

La aplicación total de la IA en la gestión y monitorización de la actividad empresarial permitirá automatizar parte de la toma de decisiones empresariales. La IA cambiará la forma en la que las empresas e instituciones se posicionarán y el rol que tendrán en la cadena de valor del ecosistema empresarial.

Además, se necesitará una regulación relativa a la automatización, destrucción de empleo, impuestos digitales…

Los retos que la IA provocará en la gestión empresarial son:

- Los trabajadores deberán aprender las nuevas habilidades para aplicar la IA.
- Es posible que se produzcan fusiones y ventas, con una creciente agregación empresarial.
- Es posible que se produzca un estancamiento de creación de nuevas compañías.

######## 3.2.6

###### Seguridad ciudadana

La mala utilización de la IA puede facilitar herramientas a criminales, piratas y delincuentes de guante blanco para robar, intimidar y manipular a cualquiera, incluyendo a gobiernos, medios de comunicación o personas.

Para esto es imprescindible desarrollar los mecanismos necesarios de seguridad, privacidad y derechos digitales.

---

######## 3.2.7

###### Escenarios extremos

En general las prospecciones de los expertos del foro ha sido optimistas y moderadas, pero a continuación enumeramos las predicciones de futuro más radicales y, salvo alguna excepción, pesimistas:

- Habrá un nuevo invierno de la IA a consecuencia del rechazo social hacia este conjunto de tecnologías.
- Dependencia emocional de la IA en una relación codependiente del humano con esta.
- IA frustrada: su desarrollo se verá bloqueado por normas arbitrarias que dan respuesta a las reclamaciones de la sociedad.
- Ciberrivalidad entre Estados: la competición entre los gobiernos por el desarrollo de la IA se convierte en una carrera militar.
- El desarrollo de la IA conducirá a un mundo fracturado con una sociedad desestructurada.
- Cibercatástrofe por ciberataque.
- Colonialismo algorítmico. Subyugación colonial de las potencias tecnológicas sobre el resto de los países. Europa se queda atrás.
- Se generaliza el Estado de la vigilancia basado en un sistema de crédito social que controla el comportamiento de los ciudadanos y su conocimiento.
- ‘AIstocracy’: la IA se convierte en un artículo de lujo.
- La IA e internet se solapan en una nueva red: Net-AI.
- El poder de las naciones se ve eclipsado por el de los grandes proveedores de IA y tecnología.
- Integración con realidades extendidas y otras tecnologías que darán lugar a especies trans- humanas.
- Economía de la abundancia. La eficiencia productiva, manufacturera, logística, comercial, etc., gracias a la IA, reduce el coste de todos  
los bienes y servicios necesarios para tener una buena calidad de vida.

---

######## 04

## Recomendaciones para IA

#### A modo de cierre de este informe, extraemos recomendaciones para un desarrollo justo y ético de la IA que enfatice en su potencial para el bien y para la generación de riqueza en el futuro:

1. El humano debe ser siempre la prioridad y estar siempre en el centro en la concepción, diseño, desarrollo e implementación de la IA. Su función es aumentar al humano, no reemplazarlo.
2. Para aprovechar e integrar la IA en cualquier entidad, empresa, organización o sistema es imprescindible la colaboración entre humanos y máquinas, o entre humanos y *software*. Como consecuencia, deben elaborarse estrategias de formación e integración en las que las personas perciban el beneficio mutuo y sientan que sus capacidades se ven aumentadas.
3. En el ámbito corporativo, es vital conocer cómo la IA puede transformar las capacidades de negocio, optimizar, ayudar a hacer predicciones y crear valor de marca a través de su integración y desarrollo.
4. En el ámbito de gobierno y social es un deber aprovechar las posibilidades que brinda la IA en diferentes áreas y disciplinas para mejorar la gestión de los recursos, la movilidad, la educación, la salud y la calidad de vida de las personas; para agilizar procesos y prestar servicios de forma más eficiente; y para hacer frente a los grandes retos globales, en especial los Objetivos de Desarrollo Sostenible (ODS).
5. Al tiempo que se aprovechan las posibilidades de la IA se deben anticipar y prevenir sus riesgos. Algunas recomendaciones para un desarrollo ético de la IA:
**a.** Afrontar los sesgos  
**b.** Poner a las personas en el centro, tratar de aumentarlas, y no de sustituirlas
**c.** Tratar de prever sus posibles efectos secundarios y terciarios indeseados  
**d.** Transparencia  
**e.** Rendición de cuentas: auditorías externas  
**f.** Mejores diseños  
**g.** Políticas de privacidad más comprensibles  
**h.** Desarrollo de algoritmos intrínsecamente explicables, que permitan saber cómo un sistema de IA ha llegado a un resultado determinado  
**i.** Debate, toma de decisiones y supervisión pública  
**j.** Educar en, con y sobre la IA  
**k.** Ampliar y velar por el cumplimiento de los derechos digitales  
**l.** Atribuir la responsabilidad debida a las empresas para que ejerzan una gestión y supervisión adecuadas de los riesgos que estas tecnologías puedan provocar y para que afronten los daños en caso de que sucedan. Que paguen por los costes sociales que su actividad genera
**m.** Explorar nuevas formas de redistribución de riqueza y de la propiedad de los datos  
**n.** Potenciar el desarrollo de la IA para el bien social  
**o.** Exigir el desarrollo de algoritmos de tipo ‘FATEN’, siglas para *fairness* (justicia), *autonomy*, *accountability* y *augmented* (autonomía, rendición de cuentas y aumento), *trust* y *transparency* (confianza y transparencia); *education* y *beneficence* (educación y beneficencia), y *non-maleficence* o no maleficencia

---

#### Agradecimientos

Nuestro agradecimiento a **Dª Esther Paniagua,** periodista y autora de este informe, donde se plasman el análisis y recomendaciones de los expertos del Future Trends Forum sobre esta tendencia.

Nuestro agradecimiento a todos los miembros del FTF asistentes a la XXXII reunión y a los colaboradores en la organización y metodología de la reunión:

######### Christopher Meyer

######### Garrick Jones
######### Clemens Hackl
######### Georg Seiler

######### Fernando de Pablo

Y por último, agradecer el compromiso del equipo de la Fundación Innovación Bankinter en nuestra misión de potenciar la innovación en nuestra sociedad:

######### Fundación Innovación Bankinter

######### Juan Moreno Bau
######### Marce Cancho
######### María Teresa Jiménez
######### Raquel Puente
######### Carmen Nestares
######### Nerea Igoa
######### Jose Carlos Huerta

Las opiniones expresadas en este informe son del autor y no reflejan la opinión de los expertos que participaron en la reunión del **Future Trends Forum.**

---

#### Ponentes y Asistentes

######### Gary Ang

Director General de Estrategia en Temasek

######### Tanisha Bassan
Desarrolladora de Quantum Computing


######### Charles Bolden

Expresidente de la NASA, Presidente de The Bolden Consulting Group LLC y Patrono de la Fundación Innovación Bankinter

######### Calum Chace

Escritor sobre IA

######### Antonio Damasio

Profesor de Neurociencia de la Universidad del Sur de California y Patrono de la Fundación Innovación Bankinter

######### Hanna Damasio

Profesora de Neurociencia y Directora del Centro Dornsife de Neurociencia de la Universidad del Sur de California

######### Soumitra Dutta

Profesor de Gestión de la Universidad de Cornell y y Patrono de la Fundación Innovación Bankinter

######### Esther Dyson

Fundadora de Wellville

######### Lauren Dyson

Data Scientist en Civis Analytics

######### Stephen Johnson

Investigador del Centro Mossavar-Rahmani para negocios y gobernanza en la Escuela Kennedy de Harvard

######### Richard Kivel

Director General de GrayBella Capital y Patrono de la Fundación Innovación Bankinter

######### Philip Lader

Asesor de Morgan Stanley y Patrono de la Fundación Innovación Bankinter

######### Jade Leung

Directora de Investigación del Centro para la Gobernanza de la Inteligencia Artificial (GovAI)

######### Anthony Lewis

Director de Inteligencia Artificial y Computación Emergente del Compute Lab de HP Labs

######### Julia Li

Fundadora y CEO de HCD Learning

######### Michael Li

Fundador y CEO de The Data Incubator

######### Qing Liu

Presidente del Instituto de Investigación de Tecnología Industrial de Jiangsu (JITRI)

######### Ramón López de Mántaras

Director del IIIA (Instituto de Investigación de Inteligencia Artificial)  
en el CSIC

######### Pilar Manchón

Vicepresidenta de IA en Roku Inc.

######### Christopher Markou

Investigador de Inteligencia Artificial en la Universidad de Cambridge y de la King’s College de Londres

######### Fiona J McEvoy
Fundadora de YouTheData.com

######### Emilio Méndez

Director del departamento de Ciencia y Tecnología de la Energía del Brookhaven National Laboratory y Patrono de la Fundación Innovación Bankinter

######### Nuria Oliver

Directora de Data Science en Data Pop Alliance  
y miembro de la Real Academia de Ingeniería de España

######### Eduardo Porter

Periodista de  
The New York Times

######### Pablo Rodríguez

Fundador de Alpha de Telefónica

######### Raúl Rojas

Profesor de IA de la Universidad Libre de Berlín

######### Juliana Rotich

Investigadora del MIT Media Lab y Patrona de la Fundación Innovación Bankinter

######### Michael Schrage

Investigador en MIT Initiative on Digital Economy

######### Eden Shochat

Socio de Aleph y Patrono de la Fundación Innovación Bankinter

######### Dor Skuler

CEO y Cofundador de Intuition Robotics

######### Oliver Smith

Director de Estrategia del Moonshot de Salud  
en Alpha, Telefónica

######### Tan Chin Nam

Exsecretario general de Singapur y Patrono de la Fundación Innovación Bankinter

######### Andrés Torrubia

CEO de Fixr.com

######### Wilfried Vanhonacker

Cofundador y Exdecano de CEIBS (Shanghai) y MSM Skolkovo (Moscú). Patrono de la Fundación Innovación Bankinter

######### Kush R Varshney

Investigador en IBM Research

######### David Weinberger

Investigador del Harvard Berkman Klein Center for Internet & Society y ponente del Google PAIR

######### Adam West

Director de Marketing de Satalia

---

Para ampliar la información sobre esta tendencia acceda a: **[fundacionbankinter.org](https://www.fundacionbankinter.org/ftf/tendencias/inteligencia-artificial)**

---

####### ES · 32

########## Fundación Innovación Bankinter

Paseo de la Castellana 29
28046 Madrid

FundacionBKT  
Fundación Innovación Bankinter
Fundación Innovación Bankinter
Fundación Innovación Bankinter
fibankinter
