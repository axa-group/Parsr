## Required dependencies
`python3 -m pip install spacy`
`python3 -m pip install markdown`
`python3 -m pip install imblearn`
`python3 -m pip install sklearn_porter`
`python3 -m pip install scikit-learn==0.22.2`
`python3 -m spacy download en_core_web_sm`

## Building the Dataset (this step is optional)

To build the dataset used to train the model run `python3 build_dataset.py <markdown folder> <json folder> <output folder>`.

**markdown folder:**\
It contains markdown files corresponding to the annotated documents. The headings are used to generate the training labels for heading detection.

**json folder:**\
It contains json files as produced by Parsr. Each file must correspond to a file in the markdown folder. It must have the same name but use the .json extension instead of .md.

**output:**\
For each document, the script generates a corresponding csv in the output folder. The csv contains the following columns:
1. line: the line as extracted by Parsr. It should correspond to a line in the pdf document.
2. is_different_style: indicates whether the line has a different font weight compare to the most common font in the document.
3. is_font_bigger: indicates whether the line has a bigger font size compare to the most common font in the document.
4. different_color: indicates whether the line has a different color compare to the most common font in the document.
5. is_font_unique: indicates whether the line has only one font.
6. text_case: text case of the line (upper case, lower case, title case or none of them)
7. word_count: number of words in the line 
8. is_number: indicates whether the line is constituted by a number
9. nb_verbs: number of verbs in the line
10. nb_nouns: number of nouns in the line
11. nb_cardinal_numbers: number of cardinal numbers in the line
12. label: "paragraph" or "heading"

```
usage: build_dataset.py [-h] md_dir json_dir out_dir

Extracts features to csv from .json files using .md files as labels

positional arguments:
  md_dir      folder containing the .md files (labels)
  json_dir    folder containing the .json files (data)
  out_dir     folder in which to save the .csv files

optional arguments:
  -h, --help  show this help message and exit
```

## Training the models

To train the model that detects headings and the one that computes their levels run `python3 train.py.py <csv folder> <output folder>`.

**Fast usage (without building the dataset):** `python3 train.py <out_dir_custom> <output folder>`

Where `out_dir_custom` is the folder containing the csv files and they are selected to obtain better performances, the folder is available on AXA-Parsr-Benchmark.

**csv folder**:\
It contains the csv generated by `build_dataset.py` and must contain csv files formatted as described previously.

**output**:\
The script generates the files `model.js` and `model_level.js` containing the executable models. This script should be placed in `server/src/processing/MlHeadingDetectionModule/train_model`.

```
usage: train.py [-h] dataset_dir out_dir

Train two decision tree models to recognize headings and compute their levels.

positional arguments:
  dataset_dir  folder containing the .csv files generated by build_dataset.py
  output_dir      folder in which to save the trained models

optional arguments:
  -h, --help   show this help message and exit
```